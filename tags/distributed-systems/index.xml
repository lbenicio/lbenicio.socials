<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Distributed-Systems on Leonardo Benicio</title><link>https://lbenicio.dev/tags/distributed-systems/</link><description>Recent content in Distributed-Systems on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sat, 27 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/distributed-systems/index.xml" rel="self" type="application/rss+xml"/><item><title>The Quiet Calculus of Probabilistic Commutativity</title><link>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</link><pubDate>Sat, 27 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Eventual consistency dominates many internet-scale systems, but reasoning about concurrency under minimal coordination remains ad hoc. This post introduces &amp;ldquo;probabilistic commutativity&amp;rdquo; — a lightweight calculus for reasoning about whether concurrent operations, under reasonable stochastic assumptions about ordering and visibility delays, are likely to commute in practice. Probabilistic commutativity offers an intermediate lens between strict algebraic commutativity and empirical test-driven guarantees, enabling low-overhead coordination strategies and probabilistic correctness arguments for producing practically consistent distributed services.&lt;/p&gt;</description></item><item><title>The Hidden Backbone of Parallelism: How Prefix Sums Power Distributed Computation</title><link>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</link><pubDate>Sun, 21 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</guid><description>&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When most people think about parallel computing, they imagine splitting a massive task into smaller chunks and running them simultaneously. That’s the Hollywood version: thousands of processors blazing through data, crunching numbers in parallel, and finishing jobs in seconds.&lt;/p&gt;
&lt;p&gt;The reality is both more fascinating and more subtle. Beneath the surface of supercomputers and distributed systems lies a set of seemingly modest mathematical operations—building blocks that make large-scale parallelism possible. Among these, one stands out for its simplicity and profound impact: the &lt;strong&gt;parallel prefix sum&lt;/strong&gt;, also called a &lt;em&gt;scan&lt;/em&gt;.&lt;/p&gt;</description></item></channel></rss>