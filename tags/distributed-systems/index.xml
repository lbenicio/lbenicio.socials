<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Distributed-Systems on Leonardo Benicio</title><link>https://lbenicio.dev/tags/distributed-systems/</link><description>Recent content in Distributed-Systems on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 26 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/distributed-systems/index.xml" rel="self" type="application/rss+xml"/><item><title>Inside Vector Databases: Building Retrieval-Augmented Systems that Scale</title><link>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</link><pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</guid><description>&lt;p&gt;Vector search used to be a research curiosity. Today it sits in the critical path of customer support bots, developer copilots, fraud monitors, and every product marketing team experimenting with &amp;ldquo;retrieval-augmented&amp;rdquo; workflows. The excitement is deserved, but so is the sober engineering required to keep these systems accurate and available. Building a production vector database is more than storing tensors and calling cosine similarity. It demands a full stack of ingestion, indexing, storage management, failure handling, evaluation, and a constant feedback loop with the language models that consume those results.&lt;/p&gt;</description></item><item><title>Distributed Systems: Consensus, Consistency, and Fault Tolerance</title><link>https://lbenicio.dev/blog/distributed-systems-consensus-consistency-and-fault-tolerance/</link><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/distributed-systems-consensus-consistency-and-fault-tolerance/</guid><description>&lt;p&gt;Distributed systems are deceptively simple to describe and maddeningly difficult to build. When a single process is replaced by a collection of cooperating processes that communicate over unreliable networks, familiar assumptions break: partial failure becomes the norm, time is not globally synchronized, and correctness requires making explicit trade-offs. This post covers the foundations you need to reason about building correct, robust, and performant distributed systems: failure models, consensus and leader election, replication and consistency models, gossip and membership, CRDTs for conflict-free replication, testing strategies (including Jepsen-style fault injection), and operational best practices.&lt;/p&gt;</description></item><item><title>The 100‑Microsecond Rule: Why Tail Latency Eats Your Throughput (and How to Fight Back)</title><link>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</guid><description>&lt;p&gt;If you stare at a performance dashboard long enough, you’ll eventually see a ghost—an outlier that refuses to go away. It’s the P99 spike that surfaces at the worst time; the one you “fix” three times and then rediscover during a product launch or a perfectly normal Tuesday.&lt;/p&gt;
&lt;p&gt;Here’s the hard truth: tail latency doesn’t just ruin your service levels; it compounds into lost throughput and broken guarantees. In systems with fan‑out, retries, and microservices, the slowest 1% isn’t “rare”—it’s the norm you ship to users most of the time. This is the 100‑microsecond rule in practice: small latencies multiply brutally at scale, and the invisible cost often starts below a single millisecond.&lt;/p&gt;</description></item><item><title>The Quiet Calculus of Probabilistic Commutativity</title><link>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</link><pubDate>Sat, 27 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Eventual consistency dominates many internet-scale systems, but reasoning about concurrency under minimal coordination remains ad hoc. This post introduces &amp;ldquo;probabilistic commutativity&amp;rdquo; — a lightweight calculus for reasoning about whether concurrent operations, under reasonable stochastic assumptions about ordering and visibility delays, are likely to commute in practice. Probabilistic commutativity offers an intermediate lens between strict algebraic commutativity and empirical test-driven guarantees, enabling low-overhead coordination strategies and probabilistic correctness arguments for producing practically consistent distributed services.&lt;/p&gt;</description></item><item><title>The Hidden Backbone of Parallelism: How Prefix Sums Power Distributed Computation</title><link>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</link><pubDate>Sun, 21 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</guid><description>&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When most people think about parallel computing, they imagine splitting a massive task into smaller chunks and running them simultaneously. That’s the Hollywood version: thousands of processors blazing through data, crunching numbers in parallel, and finishing jobs in seconds.&lt;/p&gt;
&lt;p&gt;The reality is both more fascinating and more subtle. Beneath the surface of supercomputers and distributed systems lies a set of seemingly modest mathematical operations—building blocks that make large-scale parallelism possible. Among these, one stands out for its simplicity and profound impact: the &lt;strong&gt;parallel prefix sum&lt;/strong&gt;, also called a &lt;em&gt;scan&lt;/em&gt;.&lt;/p&gt;</description></item><item><title>TCP Congestion Control: From Slow Start to BBR</title><link>https://lbenicio.dev/blog/tcp-congestion-control-from-slow-start-to-bbr/</link><pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tcp-congestion-control-from-slow-start-to-bbr/</guid><description>&lt;p&gt;Every time you load a webpage, stream a video, or download a file, TCP congestion control algorithms work invisibly to maximize throughput while preventing network collapse. These algorithms represent decades of research into one of networking&amp;rsquo;s hardest problems: how do independent senders share a network fairly without overwhelming it? Let&amp;rsquo;s explore how TCP congestion control evolved from simple beginnings to sophisticated model-based approaches.&lt;/p&gt;
&lt;h2 id="1-the-congestion-problem"&gt;1. The Congestion Problem&lt;/h2&gt;
&lt;p&gt;Without congestion control, networks collapse under load.&lt;/p&gt;</description></item><item><title>Timeouts, Retries, and Idempotency Keys: A Practical Guide</title><link>https://lbenicio.dev/blog/timeouts-retries-and-idempotency-keys-a-practical-guide/</link><pubDate>Thu, 08 Sep 2022 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/timeouts-retries-and-idempotency-keys-a-practical-guide/</guid><description>&lt;p&gt;Every distributed call needs three decisions: how long to wait, how to retry, and how to avoid duplicate effects. Done right, these three give you calm dashboards during partial failures; done wrong, they turn blips into incidents. This is a practical guide you can wire into client libraries and services without heroics.&lt;/p&gt;
&lt;h2 id="budget-your-time-not-perhop-timeouts"&gt;Budget your time, not per‑hop timeouts&lt;/h2&gt;
&lt;p&gt;Propagate a request deadline. Each hop consumes a slice and passes the remainder. Treat it as a budget: if it’s nearly spent, fail fast. Per‑hop fixed timeouts accumulate and blow your SLOs because each layer waits its entire local timeout.&lt;/p&gt;</description></item><item><title>Designing CRDT-Powered Collaboration Platforms that Stay Consistent</title><link>https://lbenicio.dev/blog/designing-crdt-powered-collaboration-platforms-that-stay-consistent/</link><pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/designing-crdt-powered-collaboration-platforms-that-stay-consistent/</guid><description>&lt;p&gt;Real-time collaboration has shifted from a feature to a baseline expectation. Teams sketch ideas in shared whiteboards, edit code together, and annotate product specs while sitting on unreliable networks. Delivering that experience demands more than WebSockets and optimistic UI code. The hard part is consistency: making sure each participant converges on the same state despite offline edits, concurrent operations, and shaky connectivity. Conflict-free replicated data types (CRDTs) emerged as the workhorse that keeps these experiences coherent.&lt;/p&gt;</description></item><item><title>Raft Fast‑Commit and PreVote in Practice</title><link>https://lbenicio.dev/blog/raft-fastcommit-and-prevote-in-practice/</link><pubDate>Mon, 09 Nov 2020 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/raft-fastcommit-and-prevote-in-practice/</guid><description>&lt;p&gt;Raft’s original paper optimizes for understandability. Production clusters optimize for availability and mean‑time‑to‑recovery. Two common extensions—PreVote and fast‑commit—reduce needless disruptions and trim the time it takes to make progress. Let’s unpack them without hand‑waving.&lt;/p&gt;
&lt;h2 id="the-pain-disruptive-elections-and-long-commit-paths"&gt;The pain: disruptive elections and long commit paths&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Without PreVote, a partitioned follower may increment term and trigger elections on rejoin, ousting a healthy leader.&lt;/li&gt;
&lt;li&gt;Without fast‑commit, clients wait for the full round‑trip alignment even when a quorum already holds the entry durably.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="prevote"&gt;PreVote&lt;/h2&gt;
&lt;p&gt;PreVote adds a “dry‑run” phase: before incrementing term, a candidate asks peers if they’d vote. Peers reject if their logs are more up‑to‑date.&lt;/p&gt;</description></item><item><title>Safe Rollback Strategies for Distributed Databases</title><link>https://lbenicio.dev/blog/safe-rollback-strategies-for-distributed-databases/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/safe-rollback-strategies-for-distributed-databases/</guid><description>&lt;p&gt;Rollbacks are the fire escapes of distributed databases. We hope never to use them, but when outages, migrations, or bad deployments hit, a well-practiced rollback can save hours of business downtime and piles of customer tickets. Unfortunately, many organizations treat rollback planning as an afterthought—&amp;ldquo;we&amp;rsquo;ll just restore from backup&amp;rdquo;—only to discover data drift, cascading failures, and angry stakeholders when the time comes. This article lays out a disciplined approach to rollback strategies tailored for modern distributed databases (PostgreSQL clusters, cloud-native NoSQL, sharded NewSQL, event-sourced systems) where consistency, latency, and regulatory obligations collide.&lt;/p&gt;</description></item><item><title>Consistent Hashing: Distributing Data Across Dynamic Clusters</title><link>https://lbenicio.dev/blog/consistent-hashing-distributing-data-across-dynamic-clusters/</link><pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/consistent-hashing-distributing-data-across-dynamic-clusters/</guid><description>&lt;p&gt;When you add a server to your distributed cache, what happens to all the cached data? With naive hashing, almost everything moves—a catastrophic reshuffling that defeats the purpose of caching. Consistent hashing solves this elegantly: only K/N keys need to move, where K is total keys and N is the number of servers. This simple idea underpins some of the most scalable systems ever built. Let&amp;rsquo;s explore how it works and why it matters.&lt;/p&gt;</description></item><item><title>Tuning the Dial: Adaptive Consistency at Planet Scale</title><link>https://lbenicio.dev/blog/tuning-the-dial-adaptive-consistency-at-planet-scale/</link><pubDate>Wed, 11 Mar 2020 14:05:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tuning-the-dial-adaptive-consistency-at-planet-scale/</guid><description>&lt;p&gt;At 2:17 a.m. UTC, a partner bank in Singapore called our incident bridge. A fund transfer appeared twice in their ledger. The culprit: a replication lag spike between Singapore and Frankfurt had stretched past our standard safety buffers. Historically we would have halted writes across the fleet, cutting availability to protect consistency. Instead, our adaptive consistency layer dialed a region-specific policy: Singapore moved from &amp;ldquo;read-after-write&amp;rdquo; to &amp;ldquo;read-your-writes&amp;rdquo; guarantees, while Frankfurt raised its commit quorum. The double posting self-corrected before social media noticed. No downtime, no irreversible loss—just a story about how we learned to treat consistency as a spectrum rather than a binary.&lt;/p&gt;</description></item><item><title>When Data Centers Learned to Sleep: Energy-Aware Scheduling in Practice</title><link>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</link><pubDate>Fri, 19 Jul 2019 09:30:00 +0000</pubDate><guid>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</guid><description>&lt;p&gt;The first time we let a data center &amp;ldquo;sleep&amp;rdquo; during daylight hours felt reckless. Customers trusted us with near-infinite elasticity. Flipping servers into deep power states to save energy sounded like penny-pinching, not engineering. Yet the math was undeniable: idling a hyperscale fleet burned as much electricity as a mid-size city. This post tells the story of how we evolved from skepticism to confidence, building energy-aware scheduling systems that keep promises while honoring planetary limits.&lt;/p&gt;</description></item></channel></rss>