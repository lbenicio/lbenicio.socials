<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine-Learning on Leonardo Benicio</title><link>https://lbenicio.dev/tags/machine-learning/</link><description>Recent content in Machine-Learning on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 26 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Inside Vector Databases: Building Retrieval-Augmented Systems that Scale</title><link>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</link><pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</guid><description>&lt;p&gt;Vector search used to be a research curiosity. Today it sits in the critical path of customer support bots, developer copilots, fraud monitors, and every product marketing team experimenting with &amp;ldquo;retrieval-augmented&amp;rdquo; workflows. The excitement is deserved, but so is the sober engineering required to keep these systems accurate and available. Building a production vector database is more than storing tensors and calling cosine similarity. It demands a full stack of ingestion, indexing, storage management, failure handling, evaluation, and a constant feedback loop with the language models that consume those results.&lt;/p&gt;</description></item><item><title>Learned Indexes: When Models Replace B‑Trees</title><link>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</guid><description>&lt;p&gt;If you’ve spent a career trusting B‑trees and hash tables, the idea of using a machine‑learned model as an index can feel like swapping a torque wrench for a Ouija board. But learned indexes aren’t a gimmick. They exploit a simple observation: real data isn’t uniformly random. It has shape—monotonic keys, skewed distributions, natural clusters—and a model can learn that shape to predict where a key lives in a sorted array. The payoff is smaller indexes, fewer cache misses, and—sometimes—dramatically faster lookups.&lt;/p&gt;</description></item><item><title>Latency-Aware Edge Inference Platforms: Engineering Consistent AI Experiences</title><link>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</guid><description>&lt;p&gt;Large language models, recommender systems, and computer vision pipelines increasingly run where users are: on kiosks, factory floors, AR headsets, connected vehicles, and retail shelves. But pushing models to the edge without a latency-aware strategy is a recipe for jittery UX, missed detections, and compliance headaches. This guide dissects what it takes to build an edge inference platform that meets strict latency budgets—even when networks wobble, models evolve weekly, and hardware varies wildly.&lt;/p&gt;</description></item><item><title>Keeping the Model Awake: Building a Self-Healing ML Inference Platform</title><link>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</link><pubDate>Tue, 14 Feb 2023 07:20:00 +0000</pubDate><guid>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</guid><description>&lt;p&gt;During a winter holiday freeze, our recommendation API refused to scale. GPUs idled waiting for models to load, autoscalers fought each other, and on-call engineers reheated leftovers at 3 a.m. while spike traffic slammed into origin. We promised leadership that this would never happen again. The solution wasn&amp;rsquo;t magic; it was a self-healing inference platform blending old-school reliability, modern ML tooling, and relentless experimentation.&lt;/p&gt;
&lt;p&gt;This post documents the rebuild. We&amp;rsquo;ll explore model packaging, warm-up rituals, adaptive scheduling, observability, chaos drills, and the social contract between ML researchers and production engineers. The goal: keep models awake, snappy, and trustworthy even when the world throws curveballs.&lt;/p&gt;</description></item><item><title>Artificial Intelligence: A Modern Approach (4th ed.)</title><link>https://lbenicio.dev/reading/artificial-intelligence-a-modern-approach-4th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/artificial-intelligence-a-modern-approach-4th-ed./</guid><description>&lt;p&gt;Definitive textbook on AI covering search, knowledge, planning, learning, and applications.&lt;/p&gt;</description></item></channel></rss>