<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Platform on Leonardo Benicio</title><link>https://lbenicio.dev/tags/platform/</link><description>Recent content in Platform on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 15 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/platform/index.xml" rel="self" type="application/rss+xml"/><item><title>Adaptive Feature Flag Frameworks for Hyper-Growth SaaS</title><link>https://lbenicio.dev/blog/adaptive-feature-flag-frameworks-for-hyper-growth-saas/</link><pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/adaptive-feature-flag-frameworks-for-hyper-growth-saas/</guid><description>&lt;p&gt;Hyper-growth SaaS companies live in perpetual motion. Every day brings new customer segments, compliance regimes, infrastructure bottlenecks, and competitive threats. Feature flags evolved from simple booleans to mission-critical systems that guard deployments, power experiments, share risk across teams, and surface real-time signals about user delight. Building an adaptive feature flag framework is no longer optional; it is the backbone of progressive delivery and organizational learning. This article offers a deep dive into the patterns, guardrails, and operating models needed to keep feature flags safe, observable, and aligned with business outcomes when your product is shipping at warp speed.&lt;/p&gt;</description></item><item><title>Keeping the Model Awake: Building a Self-Healing ML Inference Platform</title><link>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</link><pubDate>Tue, 14 Feb 2023 07:20:00 +0000</pubDate><guid>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</guid><description>&lt;p&gt;During a winter holiday freeze, our recommendation API refused to scale. GPUs idled waiting for models to load, autoscalers fought each other, and on-call engineers reheated leftovers at 3 a.m. while spike traffic slammed into origin. We promised leadership that this would never happen again. The solution wasn&amp;rsquo;t magic; it was a self-healing inference platform blending old-school reliability, modern ML tooling, and relentless experimentation.&lt;/p&gt;
&lt;p&gt;This post documents the rebuild. We&amp;rsquo;ll explore model packaging, warm-up rituals, adaptive scheduling, observability, chaos drills, and the social contract between ML researchers and production engineers. The goal: keep models awake, snappy, and trustworthy even when the world throws curveballs.&lt;/p&gt;</description></item></channel></rss>