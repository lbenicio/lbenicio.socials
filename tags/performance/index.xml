<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Performance on Leonardo Benicio</title><link>https://lbenicio.dev/tags/performance/</link><description>Recent content in Performance on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 04 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/performance/index.xml" rel="self" type="application/rss+xml"/><item><title>CPU Microarchitecture: Pipelines, Out-of-Order Execution, and Modern Performance</title><link>https://lbenicio.dev/blog/cpu-microarchitecture-pipelines-out-of-order-execution-and-modern-performance/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-microarchitecture-pipelines-out-of-order-execution-and-modern-performance/</guid><description>&lt;p&gt;Modern CPUs are marvels of engineering designed to extract instruction-level parallelism (ILP) from sequential programs while hiding long latencies — memory, multiplies, or long dependency chains. To understand why some code runs orders of magnitude faster than other code that &amp;ldquo;does the same work,&amp;rdquo; you need to understand microarchitectural components such as pipelines, superscalar issue, branch prediction, out-of-order (OoO) execution, register renaming, reorder buffers, and vector (SIMD) units. This article takes a practical tour of these features, how they affect instruction throughput and latency, and pragmatic tips for writing high-performance code.&lt;/p&gt;</description></item><item><title>Learned Indexes: When Models Replace B‑Trees</title><link>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</guid><description>&lt;p&gt;If you’ve spent a career trusting B‑trees and hash tables, the idea of using a machine‑learned model as an index can feel like swapping a torque wrench for a Ouija board. But learned indexes aren’t a gimmick. They exploit a simple observation: real data isn’t uniformly random. It has shape—monotonic keys, skewed distributions, natural clusters—and a model can learn that shape to predict where a key lives in a sorted array. The payoff is smaller indexes, fewer cache misses, and—sometimes—dramatically faster lookups.&lt;/p&gt;</description></item><item><title>The 100‑Microsecond Rule: Why Tail Latency Eats Your Throughput (and How to Fight Back)</title><link>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</guid><description>&lt;p&gt;If you stare at a performance dashboard long enough, you’ll eventually see a ghost—an outlier that refuses to go away. It’s the P99 spike that surfaces at the worst time; the one you “fix” three times and then rediscover during a product launch or a perfectly normal Tuesday.&lt;/p&gt;
&lt;p&gt;Here’s the hard truth: tail latency doesn’t just ruin your service levels; it compounds into lost throughput and broken guarantees. In systems with fan‑out, retries, and microservices, the slowest 1% isn’t “rare”—it’s the norm you ship to users most of the time. This is the 100‑microsecond rule in practice: small latencies multiply brutally at scale, and the invisible cost often starts below a single millisecond.&lt;/p&gt;</description></item><item><title>Tuning CUDA with the GPU Memory Hierarchy</title><link>https://lbenicio.dev/blog/tuning-cuda-with-the-gpu-memory-hierarchy/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tuning-cuda-with-the-gpu-memory-hierarchy/</guid><description>&lt;p&gt;CUDA performance hinges on moving data efficiently through a &lt;em&gt;hierarchy&lt;/em&gt; of memories that differ by latency, bandwidth, scope (visibility), and capacity. Raw FLOP throughput is rarely the first limiter—memory behavior, access ordering, and reuse patterns almost always dominate performance envelopes.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-the-hierarchy-at-a-glance"&gt;1. The Hierarchy at a Glance&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;Level&lt;/th&gt;
 &lt;th&gt;Scope / Visibility&lt;/th&gt;
 &lt;th&gt;Approx Latency (cycles)*&lt;/th&gt;
 &lt;th&gt;Bandwidth&lt;/th&gt;
 &lt;th&gt;Capacity (per SM / device)&lt;/th&gt;
 &lt;th&gt;Notes&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;Registers&lt;/td&gt;
 &lt;td&gt;Thread private&lt;/td&gt;
 &lt;td&gt;~1&lt;/td&gt;
 &lt;td&gt;Extreme&lt;/td&gt;
 &lt;td&gt;Tens of k per SM (allocated per thread)&lt;/td&gt;
 &lt;td&gt;Allocation affects occupancy; spilling -&amp;gt; local memory&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Shared Memory (SMEM)&lt;/td&gt;
 &lt;td&gt;Block (CTA)&lt;/td&gt;
 &lt;td&gt;~20–35&lt;/td&gt;
 &lt;td&gt;Very high&lt;/td&gt;
 &lt;td&gt;48–228 KB configurable (arch dependent)&lt;/td&gt;
 &lt;td&gt;Banked; subject to conflicts; optional split w/ L1&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;L1 / Texture Cache&lt;/td&gt;
 &lt;td&gt;SM&lt;/td&gt;
 &lt;td&gt;~30–60&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;~128–256 KB (unified)&lt;/td&gt;
 &lt;td&gt;Serves global loads; spatial locality &amp;amp; coalescing still matter&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;L2 Cache&lt;/td&gt;
 &lt;td&gt;Device-wide&lt;/td&gt;
 &lt;td&gt;~200–300&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;Multi-MB&lt;/td&gt;
 &lt;td&gt;Coherent across SMs; crucial for global data reuse&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Global DRAM&lt;/td&gt;
 &lt;td&gt;Device-wide&lt;/td&gt;
 &lt;td&gt;~400–800&lt;/td&gt;
 &lt;td&gt;High (GB/s)&lt;/td&gt;
 &lt;td&gt;Many GB&lt;/td&gt;
 &lt;td&gt;Long latency—hide with parallelism &amp;amp; coalescing&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Constant Cache&lt;/td&gt;
 &lt;td&gt;Device-wide (read-only)&lt;/td&gt;
 &lt;td&gt;~ L1 hit if cached&lt;/td&gt;
 &lt;td&gt;High (broadcast)&lt;/td&gt;
 &lt;td&gt;64 KB&lt;/td&gt;
 &lt;td&gt;Broadcast to warp if all threads read same address&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Texture / Read-Only Cache&lt;/td&gt;
 &lt;td&gt;Device-wide (cached)&lt;/td&gt;
 &lt;td&gt;Similar to L1&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;N/A&lt;/td&gt;
 &lt;td&gt;Provides specialized spatial filtering &amp;amp; relaxed coalescing&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Local Memory&lt;/td&gt;
 &lt;td&gt;Thread (spill/backing)&lt;/td&gt;
 &lt;td&gt;DRAM latency&lt;/td&gt;
 &lt;td&gt;DRAM&lt;/td&gt;
 &lt;td&gt;Per-thread virtual&lt;/td&gt;
 &lt;td&gt;“Local” is misnomer if spilled—same as global latency&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Indicative ranges; varies by architecture generation (e.g., Turing, Ampere, Hopper). Absolute numbers less important than ratio gaps.&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Lock-Free Data Structures: Concurrency Without the Wait</title><link>https://lbenicio.dev/blog/lock-free-data-structures-concurrency-without-the-wait/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/lock-free-data-structures-concurrency-without-the-wait/</guid><description>&lt;p&gt;Traditional locks have served concurrent programming for decades, but they come with costs: contention, priority inversion, and the ever-present risk of deadlock. Lock-free data structures offer an alternative—algorithms that guarantee system-wide progress even when individual threads stall. This post explores the theory, challenges, and practical implementations of lock-free programming.&lt;/p&gt;
&lt;h2 id="1-why-lock-free"&gt;1. Why Lock-Free?&lt;/h2&gt;
&lt;p&gt;Consider a simple counter shared among threads. With a mutex:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;pthread_mutex_lock&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;mutex);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;counter&lt;span style="color:#ff7b72;font-weight:bold"&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;pthread_mutex_unlock&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;mutex);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This works, but has problems:&lt;/p&gt;</description></item><item><title>Memory Allocators: From malloc to Modern Arena Allocators</title><link>https://lbenicio.dev/blog/memory-allocators-from-malloc-to-modern-arena-allocators/</link><pubDate>Thu, 14 Sep 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/memory-allocators-from-malloc-to-modern-arena-allocators/</guid><description>&lt;p&gt;Memory allocation is one of those fundamental operations that most programmers take for granted. You call &lt;code&gt;malloc()&lt;/code&gt;, memory appears; you call &lt;code&gt;free()&lt;/code&gt;, it goes away. But beneath this simple interface lies a fascinating world of algorithms, trade-offs, and optimizations that can make or break your application&amp;rsquo;s performance. This post explores memory allocation from first principles to modern high-performance allocators.&lt;/p&gt;
&lt;h2 id="1-why-memory-allocation-matters"&gt;1. Why Memory Allocation Matters&lt;/h2&gt;
&lt;p&gt;Consider a web server handling thousands of requests per second. Each request might allocate dozens of objects: strings, buffers, data structures. If each allocation takes 1 microsecond, and a request needs 50 allocations, that&amp;rsquo;s 50 microseconds just in allocation overhead—potentially more than the actual request processing time.&lt;/p&gt;</description></item><item><title>TCP Congestion Control: From Slow Start to BBR</title><link>https://lbenicio.dev/blog/tcp-congestion-control-from-slow-start-to-bbr/</link><pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tcp-congestion-control-from-slow-start-to-bbr/</guid><description>&lt;p&gt;Every time you load a webpage, stream a video, or download a file, TCP congestion control algorithms work invisibly to maximize throughput while preventing network collapse. These algorithms represent decades of research into one of networking&amp;rsquo;s hardest problems: how do independent senders share a network fairly without overwhelming it? Let&amp;rsquo;s explore how TCP congestion control evolved from simple beginnings to sophisticated model-based approaches.&lt;/p&gt;
&lt;h2 id="1-the-congestion-problem"&gt;1. The Congestion Problem&lt;/h2&gt;
&lt;p&gt;Without congestion control, networks collapse under load.&lt;/p&gt;</description></item><item><title>Garbage Collection Algorithms: From Mark-and-Sweep to ZGC</title><link>https://lbenicio.dev/blog/garbage-collection-algorithms-from-mark-and-sweep-to-zgc/</link><pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/garbage-collection-algorithms-from-mark-and-sweep-to-zgc/</guid><description>&lt;p&gt;Manual memory management is powerful but error-prone. Forget to free memory and you leak; free too early and you corrupt. Garbage collection promises to solve this by automatically reclaiming unused memory. But this convenience comes with costs and trade-offs that every systems programmer should understand. This post explores garbage collection from first principles to cutting-edge concurrent collectors.&lt;/p&gt;
&lt;h2 id="1-why-garbage-collection"&gt;1. Why Garbage Collection?&lt;/h2&gt;
&lt;p&gt;Consider the challenges of manual memory management:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#ff7b72"&gt;void&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;process_request&lt;/span&gt;(Request&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; req) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;char&lt;/span&gt;&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; buffer &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;malloc&lt;/span&gt;(&lt;span style="color:#a5d6ff"&gt;1024&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Result&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; result &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;compute&lt;/span&gt;(req, buffer);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;if&lt;/span&gt; (result&lt;span style="color:#ff7b72;font-weight:bold"&gt;-&amp;gt;&lt;/span&gt;error) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#8b949e;font-style:italic"&gt;// Oops! Forgot to free buffer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;return&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;send_response&lt;/span&gt;(result);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;free&lt;/span&gt;(buffer);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;free&lt;/span&gt;(result); &lt;span style="color:#8b949e;font-style:italic"&gt;// Did compute() allocate this? Or is it static?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Common bugs include:&lt;/p&gt;</description></item><item><title>Teaching GraphQL to Cache at the Edge</title><link>https://lbenicio.dev/blog/teaching-graphql-to-cache-at-the-edge/</link><pubDate>Sat, 03 Sep 2022 12:15:00 +0000</pubDate><guid>https://lbenicio.dev/blog/teaching-graphql-to-cache-at-the-edge/</guid><description>&lt;p&gt;GraphQL promises tailor-made responses, but tailor-made payloads resist caching. For years, we treated GraphQL responses as ephemeral: generated on demand, personalized, too unique to reuse. Then mobile latency complaints reached a boiling point. Edge locations sat underutilized while origin clusters sweated. We set out to teach GraphQL how to cache—respecting declarative queries, personalization boundaries, and real-time freshness. This is the story of building an edge caching layer that felt invisible to developers yet shaved hundreds of milliseconds off user interactions.&lt;/p&gt;</description></item><item><title>CPU Caches and Cache Coherence: The Memory Hierarchy That Makes Modern Computing Fast</title><link>https://lbenicio.dev/blog/cpu-caches-and-cache-coherence-the-memory-hierarchy-that-makes-modern-computing-fast/</link><pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-caches-and-cache-coherence-the-memory-hierarchy-that-makes-modern-computing-fast/</guid><description>&lt;p&gt;Modern CPUs can execute billions of instructions per second, but main memory takes hundreds of cycles to respond. Without caches, processors would spend most of their time waiting for data. The cache hierarchy is one of the most important innovations in computer architecture, and understanding it is essential for writing high-performance software.&lt;/p&gt;
&lt;h2 id="1-the-memory-wall-problem"&gt;1. The Memory Wall Problem&lt;/h2&gt;
&lt;p&gt;The fundamental challenge that caches solve.&lt;/p&gt;
&lt;h3 id="11-the-speed-gap"&gt;1.1 The Speed Gap&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Component Access Time Relative Speed
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;─────────────────────────────────────────────────
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;CPU Register ~0.3 ns 1x (baseline)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L1 Cache ~1 ns ~3x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L2 Cache ~3-4 ns ~10x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L3 Cache ~10-20 ns ~30-60x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Main Memory ~50-100 ns ~150-300x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;NVMe SSD ~20,000 ns ~60,000x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;HDD ~10,000,000 ns ~30,000,000x slower
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-why-the-gap-exists"&gt;1.2 Why the Gap Exists&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Memory technology tradeoffs:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;SRAM (caches):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Fast: 6 transistors per bit
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Expensive: ~100x cost per bit vs DRAM
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Power hungry
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─ Low density
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;DRAM (main memory):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Slow: 1 transistor + 1 capacitor per bit
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Cheap: high density
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Needs refresh (capacitors leak)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─ Better power per bit
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="13-the-solution-caching"&gt;1.3 The Solution: Caching&lt;/h3&gt;
&lt;p&gt;Caches exploit two key principles:&lt;/p&gt;</description></item><item><title>Virtual Memory and Page Tables: How Modern Systems Manage Memory</title><link>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-modern-systems-manage-memory/</link><pubDate>Thu, 19 May 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-modern-systems-manage-memory/</guid><description>&lt;p&gt;Every process believes it has exclusive access to a vast, contiguous memory space. This elegant illusion—virtual memory—is one of the most important abstractions in computing. Behind it lies a sophisticated system of page tables, TLBs, and hardware-software cooperation that enables memory isolation, efficient sharing, and seemingly infinite memory. Let&amp;rsquo;s explore how it all works.&lt;/p&gt;
&lt;h2 id="1-the-problem-why-virtual-memory"&gt;1. The Problem: Why Virtual Memory?&lt;/h2&gt;
&lt;p&gt;Before virtual memory, programs used physical addresses directly. This created serious problems.&lt;/p&gt;</description></item><item><title>Branch Prediction and Speculative Execution: How Modern CPUs Gamble on the Future</title><link>https://lbenicio.dev/blog/branch-prediction-and-speculative-execution-how-modern-cpus-gamble-on-the-future/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/branch-prediction-and-speculative-execution-how-modern-cpus-gamble-on-the-future/</guid><description>&lt;p&gt;Modern CPUs are marvels of prediction. Every time your code branches—every if statement, every loop iteration, every function call—the processor makes a bet on what happens next. Get it right, and execution flows at full speed. Get it wrong, and the pipeline stalls while work is thrown away. Understanding branch prediction transforms how you think about code performance. This post explores the algorithms, trade-offs, and real-world implications of one of computing&amp;rsquo;s most important optimizations.&lt;/p&gt;</description></item><item><title>B-Trees and LSM-Trees: The Foundations of Modern Storage Engines</title><link>https://lbenicio.dev/blog/b-trees-and-lsm-trees-the-foundations-of-modern-storage-engines/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/b-trees-and-lsm-trees-the-foundations-of-modern-storage-engines/</guid><description>&lt;p&gt;Every database faces the same fundamental challenge: how do you organize data on disk so that both reads and writes are fast? Two data structures have emerged as the dominant answers—B-Trees and LSM-Trees. Understanding their trade-offs is essential for anyone building or operating data-intensive systems. This post explores both in depth, from their internal mechanics to their real-world implementations.&lt;/p&gt;
&lt;h2 id="1-the-storage-engine-problem"&gt;1. The Storage Engine Problem&lt;/h2&gt;
&lt;p&gt;Before diving into specific data structures, let&amp;rsquo;s understand what we&amp;rsquo;re optimizing for.&lt;/p&gt;</description></item><item><title>CPU Caches and Memory Hierarchy: The Hidden Architecture Behind Performance</title><link>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</guid><description>&lt;p&gt;The gap between processor speed and memory speed is one of the defining challenges of modern computing. While CPUs can execute billions of operations per second, main memory takes hundreds of cycles to respond to a single request. CPU caches bridge this gap through a hierarchy of progressively larger and slower memories that exploit the patterns in how programs access data. Understanding cache behavior transforms how you think about algorithm design, data structure layout, and system performance.&lt;/p&gt;</description></item><item><title>System Calls: The Gateway Between User Space and Kernel</title><link>https://lbenicio.dev/blog/system-calls-the-gateway-between-user-space-and-kernel/</link><pubDate>Sun, 18 Apr 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/system-calls-the-gateway-between-user-space-and-kernel/</guid><description>&lt;p&gt;Every time your program opens a file, allocates memory, or sends a network packet, it crosses an invisible boundary. User programs cannot directly access hardware or kernel data structures—they must ask the operating system to do it for them through system calls. Understanding this interface is fundamental to systems programming and helps explain performance characteristics, security boundaries, and the design of operating systems themselves.&lt;/p&gt;
&lt;h2 id="1-the-user-kernel-boundary"&gt;1. The User-Kernel Boundary&lt;/h2&gt;
&lt;p&gt;Modern operating systems divide the world into two privilege levels.&lt;/p&gt;</description></item><item><title>Cache‑Friendly Data Layouts: AoS vs. SoA (and the Hybrid In‑Between)</title><link>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</link><pubDate>Thu, 18 Mar 2021 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</guid><description>&lt;p&gt;The fastest code you’ll ever write usually isn’t a brand‑new algorithm—it’s the same algorithm organized in memory so the CPU (or GPU) can consume it efficiently. That journey often starts with a deceptively small choice: Array‑of‑Structs (AoS) or Struct‑of‑Arrays (SoA). The layout you choose determines which cachelines move, which prefetchers trigger, how branch predictors behave, and whether SIMD units stay busy or starve.&lt;/p&gt;
&lt;p&gt;In this post we’ll build an intuitive mental model for cachelines, TLBs, prefetchers, and vector units; examine AoS and SoA under realistic access patterns; then derive a hybrid (AoSoA) that quietly powers many high‑performance systems—from physics engines to databases to ML dataloaders. We’ll also walk through migration strategies, benchmarking pitfalls, and checklists you can apply this week.&lt;/p&gt;</description></item><item><title>Compiler Optimizations: From Source Code to Fast Machine Code</title><link>https://lbenicio.dev/blog/compiler-optimizations-from-source-code-to-fast-machine-code/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/compiler-optimizations-from-source-code-to-fast-machine-code/</guid><description>&lt;p&gt;When you compile your code with &lt;code&gt;-O2&lt;/code&gt; or &lt;code&gt;-O3&lt;/code&gt;, something magical happens. The compiler applies dozens of optimization passes that can make your program run 10x faster—or more. Understanding these optimizations helps you write faster code and debug mysterious performance issues. Let&amp;rsquo;s explore how modern compilers transform source code into efficient machine code.&lt;/p&gt;
&lt;h2 id="1-the-compilation-pipeline"&gt;1. The Compilation Pipeline&lt;/h2&gt;
&lt;p&gt;Before diving into optimizations, let&amp;rsquo;s understand where they happen.&lt;/p&gt;
&lt;h3 id="11-compiler-phases"&gt;1.1 Compiler Phases&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Source Code
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Front-end │ Parsing, type checking, AST generation
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ (Language- │ C, C++, Rust, Swift → IR
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ specific) │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└────────┬────────┘
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Middle-end │ ★ MOST OPTIMIZATIONS HAPPEN HERE ★
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ (IR-based │ Constant folding, inlining, loop opts
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ optimization) │ Dead code elimination, vectorization
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└────────┬────────┘
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Back-end │ Instruction selection, register allocation
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ (Target- │ Instruction scheduling, peephole opts
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ specific) │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└────────┬────────┘
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Machine Code
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-intermediate-representation-ir"&gt;1.2 Intermediate Representation (IR)&lt;/h3&gt;
&lt;p&gt;LLVM IR is a popular intermediate representation:&lt;/p&gt;</description></item><item><title>Speculative Prefetchers: Designing Memory Systems That Read the Future</title><link>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</link><pubDate>Thu, 14 Feb 2019 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</guid><description>&lt;p&gt;At 2:17 a.m., the on-call performance engineer watches another alert crawl across the dashboard. The new machine image promised higher throughput for a latency-sensitive analytics service, yet caches still thrash whenever end-of-day reconciliation jobs arrive. Each job walks a sparsely linked graph of customer transactions, and the CPU spends more time waiting on memory than executing instructions. &amp;ldquo;If only the hardware could guess where the program was going next,&amp;rdquo; she sighs. That daydream is the seed of speculative prefetching—the art of reading tomorrow’s memory today.&lt;/p&gt;</description></item><item><title>Computer Architecture: A Quantitative Approach (6th ed.)</title><link>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</guid><description>&lt;p&gt;Definitive textbook on modern computer architecture and performance analysis.&lt;/p&gt;</description></item><item><title>Computer Architecture: A Quantitative Approach (6th ed.)</title><link>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</guid><description>&lt;p&gt;An in-depth treatment of modern processor and system design with quantitative evaluation.&lt;/p&gt;</description></item><item><title>Computer Systems: A Programmer's Perspective (3rd ed.)</title><link>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</guid><description>&lt;p&gt;Bridges programming and computer architecture with a focus on performance and systems.&lt;/p&gt;</description></item><item><title>Computer Systems: A Programmer's Perspective (3rd ed.)</title><link>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</guid><description>&lt;p&gt;Bridges the gap between programming and computer architecture with a focus on performance and systems.&lt;/p&gt;</description></item><item><title>Improving the Scalability and Performance of a Rails Application: A Case Study with Consul</title><link>https://lbenicio.dev/publications/improving-the-scalability-and-performance-of-a-rails-application-a-case-study-with-consul/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/publications/improving-the-scalability-and-performance-of-a-rails-application-a-case-study-with-consul/</guid><description>&lt;p&gt;Case study evaluating scalability and performance improvements of a Ruby on Rails application using Consul.&lt;/p&gt;</description></item></channel></rss>