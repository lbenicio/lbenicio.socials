<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Fundamentals on Leonardo Benicio</title><link>https://lbenicio.dev/tags/fundamentals/</link><description>Recent content in Fundamentals on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 21 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/fundamentals/index.xml" rel="self" type="application/rss+xml"/><item><title>Database Internals: Storage Engines, Transactions, and Recovery</title><link>https://lbenicio.dev/blog/database-internals-storage-engines-transactions-and-recovery/</link><pubDate>Sun, 21 Dec 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/database-internals-storage-engines-transactions-and-recovery/</guid><description>&lt;p&gt;Databases are much more than SQL parsers and client libraries — their core is a storage engine that durably stores and efficiently retrieves data while preserving the guarantees applications depend upon. This article unpacks how modern databases manage on-disk data structures, coordinate concurrent access, provide transactional semantics, and recover from crashes. We&amp;rsquo;ll look at B-trees and LSM-trees, the write-ahead log, MVCC and isolation levels, recovery algorithms, and practical tuning advice for real-world systems.&lt;/p&gt;</description></item><item><title>TLS, PKI, and Secure Protocols: How Encrypted Web Traffic Works</title><link>https://lbenicio.dev/blog/tls-pki-and-secure-protocols-how-encrypted-web-traffic-works/</link><pubDate>Tue, 18 Nov 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tls-pki-and-secure-protocols-how-encrypted-web-traffic-works/</guid><description>&lt;p&gt;Encrypted network connections are the foundation of modern secure applications. TLS (Transport Layer Security) protects confidentiality, integrity, and — when properly used — authenticity for traffic between clients and servers. Behind the simple &amp;ldquo;https&amp;rdquo; URL there are layers of protocol design, public-key cryptography, symmetric ciphers, certificate chains and revocation systems, ephemeral key exchanges, and subtle deployment pitfalls. This post explores TLS in depth: the record protocol, handshakes (with a focus on TLS 1.3), certificate validation and PKI, cipher suites and AEAD, performance considerations, QUIC integration, operational security, and testing and hardening advice.&lt;/p&gt;</description></item><item><title>Distributed Systems: Consensus, Consistency, and Fault Tolerance</title><link>https://lbenicio.dev/blog/distributed-systems-consensus-consistency-and-fault-tolerance/</link><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/distributed-systems-consensus-consistency-and-fault-tolerance/</guid><description>&lt;p&gt;Distributed systems are deceptively simple to describe and maddeningly difficult to build. When a single process is replaced by a collection of cooperating processes that communicate over unreliable networks, familiar assumptions break: partial failure becomes the norm, time is not globally synchronized, and correctness requires making explicit trade-offs. This post covers the foundations you need to reason about building correct, robust, and performant distributed systems: failure models, consensus and leader election, replication and consistency models, gossip and membership, CRDTs for conflict-free replication, testing strategies (including Jepsen-style fault injection), and operational best practices.&lt;/p&gt;</description></item><item><title>Memory Allocation and Garbage Collection: How Programs Manage Memory</title><link>https://lbenicio.dev/blog/memory-allocation-and-garbage-collection-how-programs-manage-memory/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/memory-allocation-and-garbage-collection-how-programs-manage-memory/</guid><description>&lt;p&gt;Every program needs memory, and every byte eventually becomes garbage. Between allocation and collection lies a fascinating world of algorithms, data structures, and engineering trade-offs that profoundly affect application performance. Whether you&amp;rsquo;re debugging memory leaks, optimizing allocation patterns, or choosing between languages, understanding memory management internals gives you the mental models to make better decisions.&lt;/p&gt;
&lt;h2 id="1-the-memory-landscape"&gt;1. The Memory Landscape&lt;/h2&gt;
&lt;p&gt;Before diving into allocation strategies, let&amp;rsquo;s understand the terrain.&lt;/p&gt;
&lt;h3 id="11-virtual-address-space-layout"&gt;1.1 Virtual Address Space Layout&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Typical Process Memory Layout (Linux x86-64):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────────────────────┐ 0x7FFFFFFFFFFF (high)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Stack │ ← Grows downward
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ↓ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Unmapped Region │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ↑ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Heap │ ← Grows upward
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ BSS Segment │ ← Uninitialized globals
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Data Segment │ ← Initialized globals
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Text Segment │ ← Program code (read-only)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─────────────────────────────────┘ 0x400000 (low)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The stack and heap grow toward each other from opposite ends of the address space. The stack handles function call frames automatically, while the heap requires explicit management—either by the programmer or by a garbage collector.&lt;/p&gt;</description></item><item><title>Concurrency Primitives and Synchronization: From Spinlocks to Lock-Free Data Structures</title><link>https://lbenicio.dev/blog/concurrency-primitives-and-synchronization-from-spinlocks-to-lock-free-data-structures/</link><pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/concurrency-primitives-and-synchronization-from-spinlocks-to-lock-free-data-structures/</guid><description>&lt;p&gt;Concurrent programming transforms sequential code into parallel execution, unlocking the power of modern multicore processors. But with parallelism comes the challenge of coordinating access to shared resources. Race conditions lurk in code that appears correct, and subtle bugs emerge only under specific timing conditions. Understanding synchronization primitives from the hardware level up through high-level abstractions provides the foundation for writing correct, efficient concurrent programs.&lt;/p&gt;
&lt;h2 id="1-the-concurrency-challenge"&gt;1. The Concurrency Challenge&lt;/h2&gt;
&lt;p&gt;Why synchronization matters and what problems it solves.&lt;/p&gt;</description></item><item><title>Unicode and Character Encoding: From ASCII to UTF-8 and Beyond</title><link>https://lbenicio.dev/blog/unicode-and-character-encoding-from-ascii-to-utf-8-and-beyond/</link><pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/unicode-and-character-encoding-from-ascii-to-utf-8-and-beyond/</guid><description>&lt;p&gt;Text seems simple until you try to handle it correctly. A single question—&amp;ldquo;how many characters are in this string?&amp;quot;—can have multiple valid answers depending on what you mean by &amp;ldquo;character.&amp;rdquo; Understanding Unicode and character encoding is essential for any programmer working with internationalized text, file formats, network protocols, or databases.&lt;/p&gt;
&lt;h2 id="1-the-history-of-character-encoding"&gt;1. The History of Character Encoding&lt;/h2&gt;
&lt;p&gt;Before we can understand where we are, we need to know how we got here.&lt;/p&gt;</description></item><item><title>File Systems and Storage Internals: How Data Persists on Disk</title><link>https://lbenicio.dev/blog/file-systems-and-storage-internals-how-data-persists-on-disk/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/file-systems-and-storage-internals-how-data-persists-on-disk/</guid><description>&lt;p&gt;Every file you save, every application you install, every database record—all must survive power failures and system crashes. File systems provide this durability guarantee while making storage appear as a simple hierarchy of named files and directories. Behind this abstraction lies sophisticated machinery for organizing billions of bytes, recovering from failures, and optimizing access patterns. Understanding file system internals illuminates why some operations are fast and others slow, why disks fill up unexpectedly, and how your data survives the unexpected.&lt;/p&gt;</description></item><item><title>Process Scheduling and Context Switching: How Operating Systems Share the CPU</title><link>https://lbenicio.dev/blog/process-scheduling-and-context-switching-how-operating-systems-share-the-cpu/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/process-scheduling-and-context-switching-how-operating-systems-share-the-cpu/</guid><description>&lt;p&gt;A single CPU core can only execute one instruction stream at a time, yet modern systems run hundreds of processes simultaneously. This illusion of parallelism requires the operating system to rapidly switch between processes, giving each a slice of CPU time. The scheduler—the component that decides who runs next—profoundly affects system responsiveness, throughput, and fairness. Understanding scheduling reveals why your interactive applications feel smooth or sluggish and why some workloads perform better than others.&lt;/p&gt;</description></item><item><title>Virtual Memory and Page Tables: How Operating Systems Manage Memory</title><link>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-operating-systems-manage-memory/</link><pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-operating-systems-manage-memory/</guid><description>&lt;p&gt;Every process believes it has the entire machine to itself. It sees a vast, contiguous address space starting from zero, completely isolated from other processes. This illusion is virtual memory—one of the most important abstractions in computing. Understanding how operating systems and hardware collaborate to maintain this illusion reveals fundamental insights about performance, security, and system design.&lt;/p&gt;
&lt;h2 id="1-the-need-for-virtual-memory"&gt;1. The Need for Virtual Memory&lt;/h2&gt;
&lt;p&gt;Before virtual memory, programming was a constant juggling act.&lt;/p&gt;</description></item><item><title>CPU Caches and Memory Hierarchy: The Hidden Architecture Behind Performance</title><link>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</guid><description>&lt;p&gt;The gap between processor speed and memory speed is one of the defining challenges of modern computing. While CPUs can execute billions of operations per second, main memory takes hundreds of cycles to respond to a single request. CPU caches bridge this gap through a hierarchy of progressively larger and slower memories that exploit the patterns in how programs access data. Understanding cache behavior transforms how you think about algorithm design, data structure layout, and system performance.&lt;/p&gt;</description></item><item><title>Network Sockets and the TCP/IP Stack: How Data Travels Across Networks</title><link>https://lbenicio.dev/blog/network-sockets-and-the-tcp/ip-stack-how-data-travels-across-networks/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/network-sockets-and-the-tcp/ip-stack-how-data-travels-across-networks/</guid><description>&lt;p&gt;Every web request, database query, and API call travels through the network stack. The simple act of opening a connection hides layers of protocol machinery handling packet routing, reliable delivery, congestion control, and flow management. Understanding how sockets work and how data traverses the TCP/IP stack illuminates why networks behave as they do and how to build efficient networked applications.&lt;/p&gt;
&lt;h2 id="1-the-network-stack-overview"&gt;1. The Network Stack Overview&lt;/h2&gt;
&lt;p&gt;Before diving into details, let&amp;rsquo;s see the complete picture.&lt;/p&gt;</description></item></channel></rss>