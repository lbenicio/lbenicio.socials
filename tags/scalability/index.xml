<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scalability on Leonardo Benicio</title><link>https://lbenicio.dev/tags/scalability/</link><description>Recent content in Scalability on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sat, 15 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/scalability/index.xml" rel="self" type="application/rss+xml"/><item><title>Amdahl’s Law vs. Gustafson’s Law: What They Really Predict</title><link>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</guid><description>&lt;p&gt;Amdahl’s Law and Gustafson’s Law are often presented as opposites, but they model different scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amdahl assumes a fixed problem size and asks: how much faster can I make this workload with P processors when a fraction s is inherently serial? The upper bound is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_A(P) = \frac{1}{s + \frac{1-s}{P}}.$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gustafson assumes a fixed execution time and asks: given P processors, how much bigger can the problem become if parallel parts scale? The scaled speedup is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_G(P) = s + (1-s),P.$$&lt;/p&gt;</description></item><item><title>Consistent Hashing: Distributing Data Across Dynamic Clusters</title><link>https://lbenicio.dev/blog/consistent-hashing-distributing-data-across-dynamic-clusters/</link><pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/consistent-hashing-distributing-data-across-dynamic-clusters/</guid><description>&lt;p&gt;When you add a server to your distributed cache, what happens to all the cached data? With naive hashing, almost everything moves—a catastrophic reshuffling that defeats the purpose of caching. Consistent hashing solves this elegantly: only K/N keys need to move, where K is total keys and N is the number of servers. This simple idea underpins some of the most scalable systems ever built. Let&amp;rsquo;s explore how it works and why it matters.&lt;/p&gt;</description></item><item><title>Consul: Rails Web Application Scalability and Performance Improvements with Distributed Computation</title><link>https://lbenicio.dev/publications/consul-rails-web-application-scalability-and-performance-improvements-with-distributed-computation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/publications/consul-rails-web-application-scalability-and-performance-improvements-with-distributed-computation/</guid><description>&lt;p&gt;Undergraduate monograph at IME-USP (Computer Science). Focuses on improving scalability and performance of a Rails application using distributed computation.&lt;/p&gt;</description></item><item><title>Improving the Scalability and Performance of a Rails Application: A Case Study with Consul</title><link>https://lbenicio.dev/publications/improving-the-scalability-and-performance-of-a-rails-application-a-case-study-with-consul/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/publications/improving-the-scalability-and-performance-of-a-rails-application-a-case-study-with-consul/</guid><description>&lt;p&gt;Case study evaluating scalability and performance improvements of a Ruby on Rails application using Consul.&lt;/p&gt;</description></item></channel></rss>