<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Concurrency on Leonardo Benicio</title><link>https://lbenicio.dev/tags/concurrency/</link><description>Recent content in Concurrency on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 12 Feb 2025 10:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/concurrency/index.xml" rel="self" type="application/rss+xml"/><item><title>Scheduling: Trading Latency for Throughput (and Back Again)</title><link>https://lbenicio.dev/blog/scheduling-trading-latency-for-throughput-and-back-again/</link><pubDate>Wed, 12 Feb 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/scheduling-trading-latency-for-throughput-and-back-again/</guid><description>&lt;p&gt;Schedulers encode policy: who runs next, on which core, and for how long. Those choices shuffle latency and throughput. Let’s make the trade‑offs explicit.&lt;/p&gt;
&lt;h2 id="fifo-vs-priority-vs-fair"&gt;FIFO vs. priority vs. fair&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;FIFO: simple, minimal overhead; tail prone under bursty arrivals.&lt;/li&gt;
&lt;li&gt;Priority: protects critical work; risks starvation without aging.&lt;/li&gt;
&lt;li&gt;Fair (CFQ‑like): shares CPU evenly; may underutilize when work is imbalanced.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="work-stealing"&gt;Work stealing&lt;/h2&gt;
&lt;p&gt;Great for irregular parallel workloads. Each worker has a deque; thieves steal from the tail. Pros: high utilization; Cons: cache locality losses and noisy tails under high contention.&lt;/p&gt;</description></item><item><title>Lock-Free Data Structures: Concurrency Without the Wait</title><link>https://lbenicio.dev/blog/lock-free-data-structures-concurrency-without-the-wait/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/lock-free-data-structures-concurrency-without-the-wait/</guid><description>&lt;p&gt;Traditional locks have served concurrent programming for decades, but they come with costs: contention, priority inversion, and the ever-present risk of deadlock. Lock-free data structures offer an alternative—algorithms that guarantee system-wide progress even when individual threads stall. This post explores the theory, challenges, and practical implementations of lock-free programming.&lt;/p&gt;
&lt;h2 id="1-why-lock-free"&gt;1. Why Lock-Free?&lt;/h2&gt;
&lt;p&gt;Consider a simple counter shared among threads. With a mutex:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;pthread_mutex_lock&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;mutex);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;counter&lt;span style="color:#ff7b72;font-weight:bold"&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;pthread_mutex_unlock&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;mutex);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This works, but has problems:&lt;/p&gt;</description></item><item><title>Concurrency Primitives and Synchronization: From Spinlocks to Lock-Free Data Structures</title><link>https://lbenicio.dev/blog/concurrency-primitives-and-synchronization-from-spinlocks-to-lock-free-data-structures/</link><pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/concurrency-primitives-and-synchronization-from-spinlocks-to-lock-free-data-structures/</guid><description>&lt;p&gt;Concurrent programming transforms sequential code into parallel execution, unlocking the power of modern multicore processors. But with parallelism comes the challenge of coordinating access to shared resources. Race conditions lurk in code that appears correct, and subtle bugs emerge only under specific timing conditions. Understanding synchronization primitives from the hardware level up through high-level abstractions provides the foundation for writing correct, efficient concurrent programs.&lt;/p&gt;
&lt;h2 id="1-the-concurrency-challenge"&gt;1. The Concurrency Challenge&lt;/h2&gt;
&lt;p&gt;Why synchronization matters and what problems it solves.&lt;/p&gt;</description></item><item><title>Garbage Collection Algorithms: From Mark-and-Sweep to ZGC</title><link>https://lbenicio.dev/blog/garbage-collection-algorithms-from-mark-and-sweep-to-zgc/</link><pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/garbage-collection-algorithms-from-mark-and-sweep-to-zgc/</guid><description>&lt;p&gt;Manual memory management is powerful but error-prone. Forget to free memory and you leak; free too early and you corrupt. Garbage collection promises to solve this by automatically reclaiming unused memory. But this convenience comes with costs and trade-offs that every systems programmer should understand. This post explores garbage collection from first principles to cutting-edge concurrent collectors.&lt;/p&gt;
&lt;h2 id="1-why-garbage-collection"&gt;1. Why Garbage Collection?&lt;/h2&gt;
&lt;p&gt;Consider the challenges of manual memory management:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#ff7b72"&gt;void&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;process_request&lt;/span&gt;(Request&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; req) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;char&lt;/span&gt;&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; buffer &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;malloc&lt;/span&gt;(&lt;span style="color:#a5d6ff"&gt;1024&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Result&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; result &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;compute&lt;/span&gt;(req, buffer);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;if&lt;/span&gt; (result&lt;span style="color:#ff7b72;font-weight:bold"&gt;-&amp;gt;&lt;/span&gt;error) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#8b949e;font-style:italic"&gt;// Oops! Forgot to free buffer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;return&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;send_response&lt;/span&gt;(result);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;free&lt;/span&gt;(buffer);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;free&lt;/span&gt;(result); &lt;span style="color:#8b949e;font-style:italic"&gt;// Did compute() allocate this? Or is it static?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Common bugs include:&lt;/p&gt;</description></item><item><title>Process Scheduling and Context Switching: How Operating Systems Share the CPU</title><link>https://lbenicio.dev/blog/process-scheduling-and-context-switching-how-operating-systems-share-the-cpu/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/process-scheduling-and-context-switching-how-operating-systems-share-the-cpu/</guid><description>&lt;p&gt;A single CPU core can only execute one instruction stream at a time, yet modern systems run hundreds of processes simultaneously. This illusion of parallelism requires the operating system to rapidly switch between processes, giving each a slice of CPU time. The scheduler—the component that decides who runs next—profoundly affects system responsiveness, throughput, and fairness. Understanding scheduling reveals why your interactive applications feel smooth or sluggish and why some workloads perform better than others.&lt;/p&gt;</description></item><item><title>Modern Operating Systems (4th ed.)</title><link>https://lbenicio.dev/reading/modern-operating-systems-4th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/modern-operating-systems-4th-ed./</guid><description>&lt;h2 id="a-comprehensive-os-textbook-covering-processes-memory-management-file-systems-and-security"&gt;A comprehensive OS textbook covering processes, memory management, file systems, and security&lt;/h2&gt;
&lt;p&gt;A comprehensive OS textbook covering processes, memory management, file systems, and security.&lt;/p&gt;</description></item><item><title>Operating System Concepts (9th ed.)</title><link>https://lbenicio.dev/reading/operating-system-concepts-9th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/operating-system-concepts-9th-ed./</guid><description>&lt;p&gt;Foundations of operating systems including processes, threads, scheduling, memory, and file systems.&lt;/p&gt;</description></item></channel></rss>