<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hardware on Leonardo Benicio</title><link>https://lbenicio.dev/tags/hardware/</link><description>Recent content in Hardware on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 12 Jul 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/hardware/index.xml" rel="self" type="application/rss+xml"/><item><title>CPU Caches and Cache Coherence: The Memory Hierarchy That Makes Modern Computing Fast</title><link>https://lbenicio.dev/blog/cpu-caches-and-cache-coherence-the-memory-hierarchy-that-makes-modern-computing-fast/</link><pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-caches-and-cache-coherence-the-memory-hierarchy-that-makes-modern-computing-fast/</guid><description>&lt;p&gt;Modern CPUs can execute billions of instructions per second, but main memory takes hundreds of cycles to respond. Without caches, processors would spend most of their time waiting for data. The cache hierarchy is one of the most important innovations in computer architecture, and understanding it is essential for writing high-performance software.&lt;/p&gt;
&lt;h2 id="1-the-memory-wall-problem"&gt;1. The Memory Wall Problem&lt;/h2&gt;
&lt;p&gt;The fundamental challenge that caches solve.&lt;/p&gt;
&lt;h3 id="11-the-speed-gap"&gt;1.1 The Speed Gap&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Component Access Time Relative Speed
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;─────────────────────────────────────────────────
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;CPU Register ~0.3 ns 1x (baseline)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L1 Cache ~1 ns ~3x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L2 Cache ~3-4 ns ~10x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L3 Cache ~10-20 ns ~30-60x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Main Memory ~50-100 ns ~150-300x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;NVMe SSD ~20,000 ns ~60,000x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;HDD ~10,000,000 ns ~30,000,000x slower
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-why-the-gap-exists"&gt;1.2 Why the Gap Exists&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Memory technology tradeoffs:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;SRAM (caches):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Fast: 6 transistors per bit
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Expensive: ~100x cost per bit vs DRAM
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Power hungry
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─ Low density
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;DRAM (main memory):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Slow: 1 transistor + 1 capacitor per bit
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Cheap: high density
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Needs refresh (capacitors leak)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─ Better power per bit
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="13-the-solution-caching"&gt;1.3 The Solution: Caching&lt;/h3&gt;
&lt;p&gt;Caches exploit two key principles:&lt;/p&gt;</description></item><item><title>CPU Caches and Memory Hierarchy: The Hidden Architecture Behind Performance</title><link>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</guid><description>&lt;p&gt;The gap between processor speed and memory speed is one of the defining challenges of modern computing. While CPUs can execute billions of operations per second, main memory takes hundreds of cycles to respond to a single request. CPU caches bridge this gap through a hierarchy of progressively larger and slower memories that exploit the patterns in how programs access data. Understanding cache behavior transforms how you think about algorithm design, data structure layout, and system performance.&lt;/p&gt;</description></item><item><title>Speculative Prefetchers: Designing Memory Systems That Read the Future</title><link>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</link><pubDate>Thu, 14 Feb 2019 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</guid><description>&lt;p&gt;At 2:17 a.m., the on-call performance engineer watches another alert crawl across the dashboard. The new machine image promised higher throughput for a latency-sensitive analytics service, yet caches still thrash whenever end-of-day reconciliation jobs arrive. Each job walks a sparsely linked graph of customer transactions, and the CPU spends more time waiting on memory than executing instructions. &amp;ldquo;If only the hardware could guess where the program was going next,&amp;rdquo; she sighs. That daydream is the seed of speculative prefetching—the art of reading tomorrow’s memory today.&lt;/p&gt;</description></item><item><title>Computer Architecture: A Quantitative Approach (6th ed.)</title><link>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</guid><description>&lt;p&gt;An in-depth treatment of modern processor and system design with quantitative evaluation.&lt;/p&gt;</description></item></channel></rss>