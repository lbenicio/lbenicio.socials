<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hpc on Leonardo Benicio</title><link>https://lbenicio.dev/tags/hpc/</link><description>Recent content in Hpc on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 21 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/hpc/index.xml" rel="self" type="application/rss+xml"/><item><title>The Hidden Backbone of Parallelism: How Prefix Sums Power Distributed Computation</title><link>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</link><pubDate>Sun, 21 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</guid><description>&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When most people think about parallel computing, they imagine splitting a massive task into smaller chunks and running them simultaneously. That’s the Hollywood version: thousands of processors blazing through data, crunching numbers in parallel, and finishing jobs in seconds.&lt;/p&gt;
&lt;p&gt;The reality is both more fascinating and more subtle. Beneath the surface of supercomputers and distributed systems lies a set of seemingly modest mathematical operations—building blocks that make large-scale parallelism possible. Among these, one stands out for its simplicity and profound impact: the &lt;strong&gt;parallel prefix sum&lt;/strong&gt;, also called a &lt;em&gt;scan&lt;/em&gt;.&lt;/p&gt;</description></item><item><title>GPUDirect Storage in 2025: Optimizing the End-to-End Data Path</title><link>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</guid><description>&lt;p&gt;High-performance analytics and training pipelines increasingly hinge on how &lt;em&gt;fast&lt;/em&gt; and &lt;em&gt;efficiently&lt;/em&gt; data reaches GPU memory. Compute has outpaced I/O: a single multi-GPU node can sustain tens of TFLOPs while starved by a few misconfigured storage or copy stages. GPUDirect Storage (GDS) extends the GPUDirect family (peer-to-peer, RDMA) to allow DMA engines (NVMe, NIC) to move bytes directly between storage and GPU memory—bypassing redundant copies through host DRAM and reducing CPU intervention.&lt;/p&gt;</description></item><item><title>MPI vs. OpenMP in 2025: Where Each Wins</title><link>https://lbenicio.dev/blog/mpi-vs.-openmp-in-2025-where-each-wins/</link><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/mpi-vs.-openmp-in-2025-where-each-wins/</guid><description>&lt;p&gt;Modern clusters have fat nodes (many cores, large memory) and fast interconnects. That’s why hybrid patterns—MPI between nodes and OpenMP within a node—are common.&lt;/p&gt;
&lt;h3 id="when-mpi-wins"&gt;When MPI wins&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Distributed memory by necessity: datasets exceed a node’s RAM.&lt;/li&gt;
&lt;li&gt;Clear data ownership and minimal sharing.&lt;/li&gt;
&lt;li&gt;Coarse-grained decomposition with small surface/volume ratio.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="when-openmp-wins"&gt;When OpenMP wins&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shared-memory parallel loops and tasks with modest synchronization.&lt;/li&gt;
&lt;li&gt;NUMA-aware data placement still local to a node.&lt;/li&gt;
&lt;li&gt;Rapid prototyping and incremental parallelization of CPU-bound kernels.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="hybrid-design-tips"&gt;Hybrid design tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bind MPI ranks to NUMA domains; spawn OpenMP threads within each. Avoid oversubscription.&lt;/li&gt;
&lt;li&gt;Use non-blocking collectives (MPI_Iallreduce) to hide latency.&lt;/li&gt;
&lt;li&gt;Pin memory and leverage huge pages where available.&lt;/li&gt;
&lt;li&gt;Profile: check MPI time vs. compute vs. OpenMP overhead; fix the biggest slice first.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="example-openmp-reduction-vs-mpi-allreduce"&gt;Example: OpenMP reduction vs. MPI allreduce&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-style:italic"&gt;// OpenMP reduction inside a rank
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-weight:bold;font-style:italic"&gt;#pragma omp parallel for reduction(+:sum)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#ff7b72"&gt;for&lt;/span&gt; (&lt;span style="color:#ff7b72"&gt;int&lt;/span&gt; i &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#a5d6ff"&gt;0&lt;/span&gt;; i &lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;lt;&lt;/span&gt; n; &lt;span style="color:#ff7b72;font-weight:bold"&gt;++&lt;/span&gt;i) sum &lt;span style="color:#ff7b72;font-weight:bold"&gt;+=&lt;/span&gt; a[i] &lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; b[i];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-style:italic"&gt;// Then global reduce across ranks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;MPI_Allreduce&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;sum, &lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;global, &lt;span style="color:#a5d6ff"&gt;1&lt;/span&gt;, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Rule of thumb: start simple (single model) and add hybrid complexity only when measurements show you must.&lt;/p&gt;</description></item><item><title>Amdahl’s Law vs. Gustafson’s Law: What They Really Predict</title><link>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</guid><description>&lt;p&gt;Amdahl’s Law and Gustafson’s Law are often presented as opposites, but they model different scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amdahl assumes a fixed problem size and asks: how much faster can I make this workload with P processors when a fraction s is inherently serial? The upper bound is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_A(P) = \frac{1}{s + \frac{1-s}{P}}.$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gustafson assumes a fixed execution time and asks: given P processors, how much bigger can the problem become if parallel parts scale? The scaled speedup is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_G(P) = s + (1-s),P.$$&lt;/p&gt;</description></item><item><title>Optimizing HPC Deployment: Enhancing Accessibility and Efficiency through the OMPC Framework</title><link>https://lbenicio.dev/publications/optimizing-hpc-deployment-enhancing-accessibility-and-efficiency-through-the-ompc-framework/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/publications/optimizing-hpc-deployment-enhancing-accessibility-and-efficiency-through-the-ompc-framework/</guid><description>&lt;p&gt;Paper presented at ERAD-SP 2024. Proposes the OMPC framework to improve accessibility and efficiency for HPC deployments.&lt;/p&gt;</description></item><item><title>Patterns for Parallel Programming</title><link>https://lbenicio.dev/reading/patterns-for-parallel-programming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/patterns-for-parallel-programming/</guid><description>&lt;p&gt;Pattern-based approach to structuring parallel programs across shared and distributed memory.&lt;/p&gt;</description></item></channel></rss>