<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Microarchitecture on Leonardo Benicio</title><link>https://lbenicio.dev/tags/microarchitecture/</link><description>Recent content in Microarchitecture on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 04 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/microarchitecture/index.xml" rel="self" type="application/rss+xml"/><item><title>CPU Microarchitecture: Pipelines, Out-of-Order Execution, and Modern Performance</title><link>https://lbenicio.dev/blog/cpu-microarchitecture-pipelines-out-of-order-execution-and-modern-performance/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-microarchitecture-pipelines-out-of-order-execution-and-modern-performance/</guid><description>&lt;p&gt;Modern CPUs are marvels of engineering designed to extract instruction-level parallelism (ILP) from sequential programs while hiding long latencies — memory, multiplies, or long dependency chains. To understand why some code runs orders of magnitude faster than other code that &amp;ldquo;does the same work,&amp;rdquo; you need to understand microarchitectural components such as pipelines, superscalar issue, branch prediction, out-of-order (OoO) execution, register renaming, reorder buffers, and vector (SIMD) units. This article takes a practical tour of these features, how they affect instruction throughput and latency, and pragmatic tips for writing high-performance code.&lt;/p&gt;</description></item><item><title>Branch Prediction and Speculative Execution: How Modern CPUs Gamble on the Future</title><link>https://lbenicio.dev/blog/branch-prediction-and-speculative-execution-how-modern-cpus-gamble-on-the-future/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/branch-prediction-and-speculative-execution-how-modern-cpus-gamble-on-the-future/</guid><description>&lt;p&gt;Modern CPUs are marvels of prediction. Every time your code branches—every if statement, every loop iteration, every function call—the processor makes a bet on what happens next. Get it right, and execution flows at full speed. Get it wrong, and the pipeline stalls while work is thrown away. Understanding branch prediction transforms how you think about code performance. This post explores the algorithms, trade-offs, and real-world implications of one of computing&amp;rsquo;s most important optimizations.&lt;/p&gt;</description></item><item><title>Speculative Prefetchers: Designing Memory Systems That Read the Future</title><link>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</link><pubDate>Thu, 14 Feb 2019 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</guid><description>&lt;p&gt;At 2:17 a.m., the on-call performance engineer watches another alert crawl across the dashboard. The new machine image promised higher throughput for a latency-sensitive analytics service, yet caches still thrash whenever end-of-day reconciliation jobs arrive. Each job walks a sparsely linked graph of customer transactions, and the CPU spends more time waiting on memory than executing instructions. &amp;ldquo;If only the hardware could guess where the program was going next,&amp;rdquo; she sighs. That daydream is the seed of speculative prefetching—the art of reading tomorrow’s memory today.&lt;/p&gt;</description></item></channel></rss>