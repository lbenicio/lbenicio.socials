<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Storage on Leonardo Benicio</title><link>https://lbenicio.dev/tags/storage/</link><description>Recent content in Storage on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 21 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/storage/index.xml" rel="self" type="application/rss+xml"/><item><title>Database Internals: Storage Engines, Transactions, and Recovery</title><link>https://lbenicio.dev/blog/database-internals-storage-engines-transactions-and-recovery/</link><pubDate>Sun, 21 Dec 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/database-internals-storage-engines-transactions-and-recovery/</guid><description>&lt;p&gt;Databases are much more than SQL parsers and client libraries — their core is a storage engine that durably stores and efficiently retrieves data while preserving the guarantees applications depend upon. This article unpacks how modern databases manage on-disk data structures, coordinate concurrent access, provide transactional semantics, and recover from crashes. We&amp;rsquo;ll look at B-trees and LSM-trees, the write-ahead log, MVCC and isolation levels, recovery algorithms, and practical tuning advice for real-world systems.&lt;/p&gt;</description></item><item><title>Learned Indexes: When Models Replace B‑Trees</title><link>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</guid><description>&lt;p&gt;If you’ve spent a career trusting B‑trees and hash tables, the idea of using a machine‑learned model as an index can feel like swapping a torque wrench for a Ouija board. But learned indexes aren’t a gimmick. They exploit a simple observation: real data isn’t uniformly random. It has shape—monotonic keys, skewed distributions, natural clusters—and a model can learn that shape to predict where a key lives in a sorted array. The payoff is smaller indexes, fewer cache misses, and—sometimes—dramatically faster lookups.&lt;/p&gt;</description></item><item><title>GPUDirect Storage in 2025: Optimizing the End-to-End Data Path</title><link>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</guid><description>&lt;p&gt;High-performance analytics and training pipelines increasingly hinge on how &lt;em&gt;fast&lt;/em&gt; and &lt;em&gt;efficiently&lt;/em&gt; data reaches GPU memory. Compute has outpaced I/O: a single multi-GPU node can sustain tens of TFLOPs while starved by a few misconfigured storage or copy stages. GPUDirect Storage (GDS) extends the GPUDirect family (peer-to-peer, RDMA) to allow DMA engines (NVMe, NIC) to move bytes directly between storage and GPU memory—bypassing redundant copies through host DRAM and reducing CPU intervention.&lt;/p&gt;</description></item><item><title>Write-Ahead Logging: The Unsung Hero of Database Durability</title><link>https://lbenicio.dev/blog/write-ahead-logging-the-unsung-hero-of-database-durability/</link><pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/write-ahead-logging-the-unsung-hero-of-database-durability/</guid><description>&lt;p&gt;Every database makes a promise: your committed data will survive crashes, power failures, and hardware hiccups. This promise—durability—seems simple until you consider the physics. Disks are slow. Memory is volatile. Writes can be reordered. Yet somehow, databases deliver both durability and performance. The secret weapon? Write-ahead logging.&lt;/p&gt;
&lt;h2 id="1-the-durability-problem"&gt;1. The Durability Problem&lt;/h2&gt;
&lt;p&gt;Consider what happens when you execute &lt;code&gt;UPDATE accounts SET balance = balance - 100 WHERE id = 42&lt;/code&gt;. The database must:&lt;/p&gt;</description></item><item><title>File Systems and Storage Internals: How Data Persists on Disk</title><link>https://lbenicio.dev/blog/file-systems-and-storage-internals-how-data-persists-on-disk/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/file-systems-and-storage-internals-how-data-persists-on-disk/</guid><description>&lt;p&gt;Every file you save, every application you install, every database record—all must survive power failures and system crashes. File systems provide this durability guarantee while making storage appear as a simple hierarchy of named files and directories. Behind this abstraction lies sophisticated machinery for organizing billions of bytes, recovering from failures, and optimizing access patterns. Understanding file system internals illuminates why some operations are fast and others slow, why disks fill up unexpectedly, and how your data survives the unexpected.&lt;/p&gt;</description></item><item><title>Merkle Trees and Content‑Addressable Storage</title><link>https://lbenicio.dev/blog/merkle-trees-and-contentaddressable-storage/</link><pubDate>Mon, 17 Aug 2020 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/merkle-trees-and-contentaddressable-storage/</guid><description>&lt;p&gt;Hash the content, not the location—that’s the core of content‑addressable storage (CAS). Combine it with Merkle trees (or DAGs) and you get efficient verification, deduplication, and synchronization. This post connects the dots from Git to large‑scale object stores.&lt;/p&gt;
&lt;h2 id="why-merkle"&gt;Why Merkle?&lt;/h2&gt;
&lt;p&gt;Parent hashes commit to child hashes; any change percolates up. You can verify integrity by checking a root hash and a short proof path.&lt;/p&gt;
&lt;h2 id="uses"&gt;Uses&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Git commits and trees; shallow clones via missing subtrees.&lt;/li&gt;
&lt;li&gt;Package managers with integrity checks.&lt;/li&gt;
&lt;li&gt;Deduplicated backups with chunking and rolling hashes.&lt;/li&gt;
&lt;li&gt;Object stores that replicate by exchanging missing subgraphs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="practical-notes"&gt;Practical notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hash choice (SHA‑256 vs BLAKE3) affects speed and hardware support.&lt;/li&gt;
&lt;li&gt;Chunking strategy controls dedupe granularity; rolling fingerprints (Rabin) find natural boundaries.&lt;/li&gt;
&lt;li&gt;Store metadata alongside blobs to avoid rehashing for trivial changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="1-from-trees-to-dags-modeling-versions-that-share-content"&gt;1) From trees to DAGs: modeling versions that share content&lt;/h2&gt;
&lt;p&gt;Classic Merkle trees have a fixed arity and two kinds of nodes: leaves containing hashes of fixed‑size blocks, and internal nodes containing hashes of their children. In real systems, multiple versions of content share substructure: two versions of a directory share most files, two backups share most chunks, two container images share many layers. That sharing naturally forms a directed acyclic graph (DAG) where a node can be referenced from multiple parents. The root is still a commitment to the whole, but now many roots can reference common subgraphs without duplication.&lt;/p&gt;</description></item></channel></rss>