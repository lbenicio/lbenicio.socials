<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cache on Leonardo Benicio</title><link>https://lbenicio.dev/tags/cache/</link><description>Recent content in Cache on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 12 Jul 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/cache/index.xml" rel="self" type="application/rss+xml"/><item><title>CPU Caches and Cache Coherence: The Memory Hierarchy That Makes Modern Computing Fast</title><link>https://lbenicio.dev/blog/cpu-caches-and-cache-coherence-the-memory-hierarchy-that-makes-modern-computing-fast/</link><pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-caches-and-cache-coherence-the-memory-hierarchy-that-makes-modern-computing-fast/</guid><description>&lt;p&gt;Modern CPUs can execute billions of instructions per second, but main memory takes hundreds of cycles to respond. Without caches, processors would spend most of their time waiting for data. The cache hierarchy is one of the most important innovations in computer architecture, and understanding it is essential for writing high-performance software.&lt;/p&gt;
&lt;h2 id="1-the-memory-wall-problem"&gt;1. The Memory Wall Problem&lt;/h2&gt;
&lt;p&gt;The fundamental challenge that caches solve.&lt;/p&gt;
&lt;h3 id="11-the-speed-gap"&gt;1.1 The Speed Gap&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Component Access Time Relative Speed
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;─────────────────────────────────────────────────
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;CPU Register ~0.3 ns 1x (baseline)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L1 Cache ~1 ns ~3x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L2 Cache ~3-4 ns ~10x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L3 Cache ~10-20 ns ~30-60x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Main Memory ~50-100 ns ~150-300x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;NVMe SSD ~20,000 ns ~60,000x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;HDD ~10,000,000 ns ~30,000,000x slower
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-why-the-gap-exists"&gt;1.2 Why the Gap Exists&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Memory technology tradeoffs:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;SRAM (caches):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Fast: 6 transistors per bit
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Expensive: ~100x cost per bit vs DRAM
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Power hungry
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─ Low density
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;DRAM (main memory):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Slow: 1 transistor + 1 capacitor per bit
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Cheap: high density
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Needs refresh (capacitors leak)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─ Better power per bit
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="13-the-solution-caching"&gt;1.3 The Solution: Caching&lt;/h3&gt;
&lt;p&gt;Caches exploit two key principles:&lt;/p&gt;</description></item><item><title>CPU Caches and Memory Hierarchy: The Hidden Architecture Behind Performance</title><link>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</guid><description>&lt;p&gt;The gap between processor speed and memory speed is one of the defining challenges of modern computing. While CPUs can execute billions of operations per second, main memory takes hundreds of cycles to respond to a single request. CPU caches bridge this gap through a hierarchy of progressively larger and slower memories that exploit the patterns in how programs access data. Understanding cache behavior transforms how you think about algorithm design, data structure layout, and system performance.&lt;/p&gt;</description></item><item><title>Cache‑Friendly Data Layouts: AoS vs. SoA (and the Hybrid In‑Between)</title><link>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</link><pubDate>Thu, 18 Mar 2021 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</guid><description>&lt;p&gt;The fastest code you’ll ever write usually isn’t a brand‑new algorithm—it’s the same algorithm organized in memory so the CPU (or GPU) can consume it efficiently. That journey often starts with a deceptively small choice: Array‑of‑Structs (AoS) or Struct‑of‑Arrays (SoA). The layout you choose determines which cachelines move, which prefetchers trigger, how branch predictors behave, and whether SIMD units stay busy or starve.&lt;/p&gt;
&lt;p&gt;In this post we’ll build an intuitive mental model for cachelines, TLBs, prefetchers, and vector units; examine AoS and SoA under realistic access patterns; then derive a hybrid (AoSoA) that quietly powers many high‑performance systems—from physics engines to databases to ML dataloaders. We’ll also walk through migration strategies, benchmarking pitfalls, and checklists you can apply this week.&lt;/p&gt;</description></item></channel></rss>