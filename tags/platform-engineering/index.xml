<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Platform-Engineering on Leonardo Benicio</title><link>https://lbenicio.dev/tags/platform-engineering/</link><description>Recent content in Platform-Engineering on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 12 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/platform-engineering/index.xml" rel="self" type="application/rss+xml"/><item><title>Latency-Aware Edge Inference Platforms: Engineering Consistent AI Experiences</title><link>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</guid><description>&lt;p&gt;Large language models, recommender systems, and computer vision pipelines increasingly run where users are: on kiosks, factory floors, AR headsets, connected vehicles, and retail shelves. But pushing models to the edge without a latency-aware strategy is a recipe for jittery UX, missed detections, and compliance headaches. This guide dissects what it takes to build an edge inference platform that meets strict latency budgetsâ€”even when networks wobble, models evolve weekly, and hardware varies wildly.&lt;/p&gt;</description></item></channel></rss>