<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Openmp on Leonardo Benicio</title><link>https://lbenicio.dev/tags/openmp/</link><description>Recent content in Openmp on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 04 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/openmp/index.xml" rel="self" type="application/rss+xml"/><item><title>MPI vs. OpenMP in 2025: Where Each Wins</title><link>https://lbenicio.dev/blog/mpi-vs.-openmp-in-2025-where-each-wins/</link><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/mpi-vs.-openmp-in-2025-where-each-wins/</guid><description>&lt;p&gt;Modern clusters have fat nodes (many cores, large memory) and fast interconnects. That’s why hybrid patterns—MPI between nodes and OpenMP within a node—are common.&lt;/p&gt;
&lt;h3 id="when-mpi-wins"&gt;When MPI wins&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Distributed memory by necessity: datasets exceed a node’s RAM.&lt;/li&gt;
&lt;li&gt;Clear data ownership and minimal sharing.&lt;/li&gt;
&lt;li&gt;Coarse-grained decomposition with small surface/volume ratio.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="when-openmp-wins"&gt;When OpenMP wins&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shared-memory parallel loops and tasks with modest synchronization.&lt;/li&gt;
&lt;li&gt;NUMA-aware data placement still local to a node.&lt;/li&gt;
&lt;li&gt;Rapid prototyping and incremental parallelization of CPU-bound kernels.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="hybrid-design-tips"&gt;Hybrid design tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bind MPI ranks to NUMA domains; spawn OpenMP threads within each. Avoid oversubscription.&lt;/li&gt;
&lt;li&gt;Use non-blocking collectives (MPI_Iallreduce) to hide latency.&lt;/li&gt;
&lt;li&gt;Pin memory and leverage huge pages where available.&lt;/li&gt;
&lt;li&gt;Profile: check MPI time vs. compute vs. OpenMP overhead; fix the biggest slice first.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="example-openmp-reduction-vs-mpi-allreduce"&gt;Example: OpenMP reduction vs. MPI allreduce&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-style:italic"&gt;// OpenMP reduction inside a rank
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-weight:bold;font-style:italic"&gt;#pragma omp parallel for reduction(+:sum)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#ff7b72"&gt;for&lt;/span&gt; (&lt;span style="color:#ff7b72"&gt;int&lt;/span&gt; i &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#a5d6ff"&gt;0&lt;/span&gt;; i &lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;lt;&lt;/span&gt; n; &lt;span style="color:#ff7b72;font-weight:bold"&gt;++&lt;/span&gt;i) sum &lt;span style="color:#ff7b72;font-weight:bold"&gt;+=&lt;/span&gt; a[i] &lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; b[i];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-style:italic"&gt;// Then global reduce across ranks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;MPI_Allreduce&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;sum, &lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;global, &lt;span style="color:#a5d6ff"&gt;1&lt;/span&gt;, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Rule of thumb: start simple (single model) and add hybrid complexity only when measurements show you must.&lt;/p&gt;</description></item><item><title>OpenMP Application Programming Interface, Version 5.2</title><link>https://lbenicio.dev/reading/openmp-application-programming-interface-version-5.2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/openmp-application-programming-interface-version-5.2/</guid><description>&lt;p&gt;Official OpenMP specification with the latest directives, memory model, and examples.&lt;/p&gt;</description></item><item><title>Optimizing HPC Deployment: Enhancing Accessibility and Efficiency through the OMPC Framework</title><link>https://lbenicio.dev/publications/optimizing-hpc-deployment-enhancing-accessibility-and-efficiency-through-the-ompc-framework/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/publications/optimizing-hpc-deployment-enhancing-accessibility-and-efficiency-through-the-ompc-framework/</guid><description>&lt;p&gt;Paper presented at ERAD-SP 2024. Proposes the OMPC framework to improve accessibility and efficiency for HPC deployments.&lt;/p&gt;</description></item><item><title>Parallel Programming in C with MPI and OpenMP</title><link>https://lbenicio.dev/reading/parallel-programming-in-c-with-mpi-and-openmp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/parallel-programming-in-c-with-mpi-and-openmp/</guid><description>&lt;p&gt;Introductory yet practical coverage of parallel programming in C using both MPI and OpenMP.&lt;/p&gt;</description></item><item><title>Using OpenMP: Portable Shared Memory Parallel Programming</title><link>https://lbenicio.dev/reading/using-openmp-portable-shared-memory-parallel-programming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/using-openmp-portable-shared-memory-parallel-programming/</guid><description>&lt;p&gt;Practical OpenMP techniques, performance tips, and patterns for shared-memory parallel programming.&lt;/p&gt;</description></item></channel></rss>