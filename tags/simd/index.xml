<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simd on Leonardo Benicio</title><link>https://lbenicio.dev/tags/simd/</link><description>Recent content in Simd on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 04 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/simd/index.xml" rel="self" type="application/rss+xml"/><item><title>CPU Microarchitecture: Pipelines, Out-of-Order Execution, and Modern Performance</title><link>https://lbenicio.dev/blog/cpu-microarchitecture-pipelines-out-of-order-execution-and-modern-performance/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-microarchitecture-pipelines-out-of-order-execution-and-modern-performance/</guid><description>&lt;p&gt;Modern CPUs are marvels of engineering designed to extract instruction-level parallelism (ILP) from sequential programs while hiding long latencies — memory, multiplies, or long dependency chains. To understand why some code runs orders of magnitude faster than other code that &amp;ldquo;does the same work,&amp;rdquo; you need to understand microarchitectural components such as pipelines, superscalar issue, branch prediction, out-of-order (OoO) execution, register renaming, reorder buffers, and vector (SIMD) units. This article takes a practical tour of these features, how they affect instruction throughput and latency, and pragmatic tips for writing high-performance code.&lt;/p&gt;</description></item><item><title>Cache‑Friendly Data Layouts: AoS vs. SoA (and the Hybrid In‑Between)</title><link>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</link><pubDate>Thu, 18 Mar 2021 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</guid><description>&lt;p&gt;The fastest code you’ll ever write usually isn’t a brand‑new algorithm—it’s the same algorithm organized in memory so the CPU (or GPU) can consume it efficiently. That journey often starts with a deceptively small choice: Array‑of‑Structs (AoS) or Struct‑of‑Arrays (SoA). The layout you choose determines which cachelines move, which prefetchers trigger, how branch predictors behave, and whether SIMD units stay busy or starve.&lt;/p&gt;
&lt;p&gt;In this post we’ll build an intuitive mental model for cachelines, TLBs, prefetchers, and vector units; examine AoS and SoA under realistic access patterns; then derive a hybrid (AoSoA) that quietly powers many high‑performance systems—from physics engines to databases to ML dataloaders. We’ll also walk through migration strategies, benchmarking pitfalls, and checklists you can apply this week.&lt;/p&gt;</description></item></channel></rss>