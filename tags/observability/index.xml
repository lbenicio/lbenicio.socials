<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Observability on Leonardo Benicio</title><link>https://lbenicio.dev/tags/observability/</link><description>Recent content in Observability on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 26 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/observability/index.xml" rel="self" type="application/rss+xml"/><item><title>Inside Vector Databases: Building Retrieval-Augmented Systems that Scale</title><link>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</link><pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</guid><description>&lt;p&gt;Vector search used to be a research curiosity. Today it sits in the critical path of customer support bots, developer copilots, fraud monitors, and every product marketing team experimenting with &amp;ldquo;retrieval-augmented&amp;rdquo; workflows. The excitement is deserved, but so is the sober engineering required to keep these systems accurate and available. Building a production vector database is more than storing tensors and calling cosine similarity. It demands a full stack of ingestion, indexing, storage management, failure handling, evaluation, and a constant feedback loop with the language models that consume those results.&lt;/p&gt;</description></item><item><title>Seeing in the Dark: Observability for Edge AI Fleets</title><link>https://lbenicio.dev/blog/seeing-in-the-dark-observability-for-edge-ai-fleets/</link><pubDate>Fri, 16 Aug 2024 10:55:00 +0000</pubDate><guid>https://lbenicio.dev/blog/seeing-in-the-dark-observability-for-edge-ai-fleets/</guid><description>&lt;p&gt;Our edge AI deployment began with a handful of pilot devices in retail stores. Within months, thousands of cameras, sensors, and point-of-sale terminals joined the fleet. They detected shelves running low, predicted queue lengths, and flagged suspicious transactions. But when a customer called asking why a device misclassified bananas as tennis balls, we realized our observability blurred at the edge. Logs vanished into the ether, metrics arrived sporadically, and models drifted silently. This article shares how we built observability robust enough for flaky networks, sensitive data, and autonomous updates.&lt;/p&gt;</description></item><item><title>Latency-Aware Edge Inference Platforms: Engineering Consistent AI Experiences</title><link>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</guid><description>&lt;p&gt;Large language models, recommender systems, and computer vision pipelines increasingly run where users are: on kiosks, factory floors, AR headsets, connected vehicles, and retail shelves. But pushing models to the edge without a latency-aware strategy is a recipe for jittery UX, missed detections, and compliance headaches. This guide dissects what it takes to build an edge inference platform that meets strict latency budgets—even when networks wobble, models evolve weekly, and hardware varies wildly.&lt;/p&gt;</description></item><item><title>Keeping the Model Awake: Building a Self-Healing ML Inference Platform</title><link>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</link><pubDate>Tue, 14 Feb 2023 07:20:00 +0000</pubDate><guid>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</guid><description>&lt;p&gt;During a winter holiday freeze, our recommendation API refused to scale. GPUs idled waiting for models to load, autoscalers fought each other, and on-call engineers reheated leftovers at 3 a.m. while spike traffic slammed into origin. We promised leadership that this would never happen again. The solution wasn&amp;rsquo;t magic; it was a self-healing inference platform blending old-school reliability, modern ML tooling, and relentless experimentation.&lt;/p&gt;
&lt;p&gt;This post documents the rebuild. We&amp;rsquo;ll explore model packaging, warm-up rituals, adaptive scheduling, observability, chaos drills, and the social contract between ML researchers and production engineers. The goal: keep models awake, snappy, and trustworthy even when the world throws curveballs.&lt;/p&gt;</description></item><item><title>Instrumenting Without Spying: Privacy-Preserving Telemetry at Scale</title><link>https://lbenicio.dev/blog/instrumenting-without-spying-privacy-preserving-telemetry-at-scale/</link><pubDate>Thu, 27 May 2021 18:45:00 +0000</pubDate><guid>https://lbenicio.dev/blog/instrumenting-without-spying-privacy-preserving-telemetry-at-scale/</guid><description>&lt;p&gt;Three years ago, our observability dashboards flickered ominously: ingestion lag, missing metrics, red error bars. We had paused telemetry from millions of devices after discovering that our pipeline collected more than we were comfortable storing. The pause kept our promise to users—but left engineers blind. We needed a way to reinstate observability without betraying trust. The result was a radical redesign: a privacy-preserving telemetry system that treats data minimization as a feature, not a compliance chore.&lt;/p&gt;</description></item><item><title>Safe Rollback Strategies for Distributed Databases</title><link>https://lbenicio.dev/blog/safe-rollback-strategies-for-distributed-databases/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/safe-rollback-strategies-for-distributed-databases/</guid><description>&lt;p&gt;Rollbacks are the fire escapes of distributed databases. We hope never to use them, but when outages, migrations, or bad deployments hit, a well-practiced rollback can save hours of business downtime and piles of customer tickets. Unfortunately, many organizations treat rollback planning as an afterthought—&amp;ldquo;we&amp;rsquo;ll just restore from backup&amp;rdquo;—only to discover data drift, cascading failures, and angry stakeholders when the time comes. This article lays out a disciplined approach to rollback strategies tailored for modern distributed databases (PostgreSQL clusters, cloud-native NoSQL, sharded NewSQL, event-sourced systems) where consistency, latency, and regulatory obligations collide.&lt;/p&gt;</description></item></channel></rss>