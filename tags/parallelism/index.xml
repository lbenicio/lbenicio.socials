<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Parallelism on Leonardo Benicio</title><link>https://lbenicio.dev/tags/parallelism/</link><description>Recent content in Parallelism on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 21 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/tags/parallelism/index.xml" rel="self" type="application/rss+xml"/><item><title>The Hidden Backbone of Parallelism: How Prefix Sums Power Distributed Computation</title><link>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</link><pubDate>Sun, 21 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</guid><description>&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When most people think about parallel computing, they imagine splitting a massive task into smaller chunks and running them simultaneously. That’s the Hollywood version: thousands of processors blazing through data, crunching numbers in parallel, and finishing jobs in seconds.&lt;/p&gt;
&lt;p&gt;The reality is both more fascinating and more subtle. Beneath the surface of supercomputers and distributed systems lies a set of seemingly modest mathematical operations—building blocks that make large-scale parallelism possible. Among these, one stands out for its simplicity and profound impact: the &lt;strong&gt;parallel prefix sum&lt;/strong&gt;, also called a &lt;em&gt;scan&lt;/em&gt;.&lt;/p&gt;</description></item><item><title>Amdahl’s Law vs. Gustafson’s Law: What They Really Predict</title><link>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</guid><description>&lt;p&gt;Amdahl’s Law and Gustafson’s Law are often presented as opposites, but they model different scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amdahl assumes a fixed problem size and asks: how much faster can I make this workload with P processors when a fraction s is inherently serial? The upper bound is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_A(P) = \frac{1}{s + \frac{1-s}{P}}.$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gustafson assumes a fixed execution time and asks: given P processors, how much bigger can the problem become if parallel parts scale? The scaled speedup is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_G(P) = s + (1-s),P.$$&lt;/p&gt;</description></item></channel></rss>