<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Parallelism on Leonardo Benicio</title><link>https://lbenicio.dev/categories/parallelism/</link><description>Recent content in Parallelism on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 27 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/categories/parallelism/index.xml" rel="self" type="application/rss+xml"/><item><title>Tuning CUDA with the GPU Memory Hierarchy</title><link>https://lbenicio.dev/blog/tuning-cuda-with-the-gpu-memory-hierarchy/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tuning-cuda-with-the-gpu-memory-hierarchy/</guid><description>&lt;p&gt;CUDA performance hinges on moving data efficiently through a &lt;em&gt;hierarchy&lt;/em&gt; of memories that differ by latency, bandwidth, scope (visibility), and capacity. Raw FLOP throughput is rarely the first limiter—memory behavior, access ordering, and reuse patterns almost always dominate performance envelopes.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-the-hierarchy-at-a-glance"&gt;1. The Hierarchy at a Glance&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;Level&lt;/th&gt;
 &lt;th&gt;Scope / Visibility&lt;/th&gt;
 &lt;th&gt;Approx Latency (cycles)*&lt;/th&gt;
 &lt;th&gt;Bandwidth&lt;/th&gt;
 &lt;th&gt;Capacity (per SM / device)&lt;/th&gt;
 &lt;th&gt;Notes&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;Registers&lt;/td&gt;
 &lt;td&gt;Thread private&lt;/td&gt;
 &lt;td&gt;~1&lt;/td&gt;
 &lt;td&gt;Extreme&lt;/td&gt;
 &lt;td&gt;Tens of k per SM (allocated per thread)&lt;/td&gt;
 &lt;td&gt;Allocation affects occupancy; spilling -&amp;gt; local memory&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Shared Memory (SMEM)&lt;/td&gt;
 &lt;td&gt;Block (CTA)&lt;/td&gt;
 &lt;td&gt;~20–35&lt;/td&gt;
 &lt;td&gt;Very high&lt;/td&gt;
 &lt;td&gt;48–228 KB configurable (arch dependent)&lt;/td&gt;
 &lt;td&gt;Banked; subject to conflicts; optional split w/ L1&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;L1 / Texture Cache&lt;/td&gt;
 &lt;td&gt;SM&lt;/td&gt;
 &lt;td&gt;~30–60&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;~128–256 KB (unified)&lt;/td&gt;
 &lt;td&gt;Serves global loads; spatial locality &amp;amp; coalescing still matter&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;L2 Cache&lt;/td&gt;
 &lt;td&gt;Device-wide&lt;/td&gt;
 &lt;td&gt;~200–300&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;Multi-MB&lt;/td&gt;
 &lt;td&gt;Coherent across SMs; crucial for global data reuse&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Global DRAM&lt;/td&gt;
 &lt;td&gt;Device-wide&lt;/td&gt;
 &lt;td&gt;~400–800&lt;/td&gt;
 &lt;td&gt;High (GB/s)&lt;/td&gt;
 &lt;td&gt;Many GB&lt;/td&gt;
 &lt;td&gt;Long latency—hide with parallelism &amp;amp; coalescing&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Constant Cache&lt;/td&gt;
 &lt;td&gt;Device-wide (read-only)&lt;/td&gt;
 &lt;td&gt;~ L1 hit if cached&lt;/td&gt;
 &lt;td&gt;High (broadcast)&lt;/td&gt;
 &lt;td&gt;64 KB&lt;/td&gt;
 &lt;td&gt;Broadcast to warp if all threads read same address&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Texture / Read-Only Cache&lt;/td&gt;
 &lt;td&gt;Device-wide (cached)&lt;/td&gt;
 &lt;td&gt;Similar to L1&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;N/A&lt;/td&gt;
 &lt;td&gt;Provides specialized spatial filtering &amp;amp; relaxed coalescing&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Local Memory&lt;/td&gt;
 &lt;td&gt;Thread (spill/backing)&lt;/td&gt;
 &lt;td&gt;DRAM latency&lt;/td&gt;
 &lt;td&gt;DRAM&lt;/td&gt;
 &lt;td&gt;Per-thread virtual&lt;/td&gt;
 &lt;td&gt;“Local” is misnomer if spilled—same as global latency&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Indicative ranges; varies by architecture generation (e.g., Turing, Ampere, Hopper). Absolute numbers less important than ratio gaps.&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>