<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Distributed Systems on Leonardo Benicio</title><link>https://lbenicio.dev/categories/distributed-systems/</link><description>Recent content in Distributed Systems on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 26 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/categories/distributed-systems/index.xml" rel="self" type="application/rss+xml"/><item><title>Inside Vector Databases: Building Retrieval-Augmented Systems that Scale</title><link>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</link><pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</guid><description>&lt;p&gt;Vector search used to be a research curiosity. Today it sits in the critical path of customer support bots, developer copilots, fraud monitors, and every product marketing team experimenting with &amp;ldquo;retrieval-augmented&amp;rdquo; workflows. The excitement is deserved, but so is the sober engineering required to keep these systems accurate and available. Building a production vector database is more than storing tensors and calling cosine similarity. It demands a full stack of ingestion, indexing, storage management, failure handling, evaluation, and a constant feedback loop with the language models that consume those results.&lt;/p&gt;</description></item><item><title>GPUDirect Storage in 2025: Optimizing the End-to-End Data Path</title><link>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</guid><description>&lt;p&gt;High-performance analytics and training pipelines increasingly hinge on how &lt;em&gt;fast&lt;/em&gt; and &lt;em&gt;efficiently&lt;/em&gt; data reaches GPU memory. Compute has outpaced I/O: a single multi-GPU node can sustain tens of TFLOPs while starved by a few misconfigured storage or copy stages. GPUDirect Storage (GDS) extends the GPUDirect family (peer-to-peer, RDMA) to allow DMA engines (NVMe, NIC) to move bytes directly between storage and GPU memory—bypassing redundant copies through host DRAM and reducing CPU intervention.&lt;/p&gt;</description></item><item><title>From MapReduce to Spark: The Arc of Data-Parallel Systems</title><link>https://lbenicio.dev/blog/from-mapreduce-to-spark-the-arc-of-data-parallel-systems/</link><pubDate>Mon, 19 May 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/from-mapreduce-to-spark-the-arc-of-data-parallel-systems/</guid><description>&lt;p&gt;MapReduce popularized large-scale batch processing with a simple model (map, shuffle, reduce) and immutable intermediate state on HDFS. It optimized for throughput and fault tolerance via re-execution.&lt;/p&gt;
&lt;p&gt;Spark expanded the model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RDDs: immutable, partitioned datasets with lineage, enabling recomputation on failure.&lt;/li&gt;
&lt;li&gt;DAG scheduler: plans multi-stage jobs, pipelining narrow transformations and materializing wide ones.&lt;/li&gt;
&lt;li&gt;In-memory caching: keeps hot datasets in RAM to accelerate iterative workloads.&lt;/li&gt;
&lt;li&gt;Higher-level APIs: DataFrames/Datasets and SQL, plus MLlib and Structured Streaming.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="checkpointing-and-lineage"&gt;Checkpointing and lineage&lt;/h3&gt;
&lt;p&gt;RDD lineage can grow large; Spark checkpoints to cut recomputation cost. For streaming, write-ahead logs plus checkpoints enable recovery.&lt;/p&gt;</description></item><item><title>Exactly-Once in Streaming: What It Means and How Systems Achieve It</title><link>https://lbenicio.dev/blog/exactly-once-in-streaming-what-it-means-and-how-systems-achieve-it/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/exactly-once-in-streaming-what-it-means-and-how-systems-achieve-it/</guid><description>&lt;p&gt;&amp;ldquo;Exactly-once&amp;rdquo; doesn’t mean an event packet traverses the network a single time. It means that in the &lt;em&gt;observable&lt;/em&gt; outputs (state, emitted records, external side effects) each logical input is reflected &lt;strong&gt;at most once&lt;/strong&gt; and &lt;strong&gt;at least once&lt;/strong&gt;—so exactly once—despite retries, replays, failovers, or speculative execution. It is an &lt;em&gt;end-to-end&lt;/em&gt; property requiring cooperation across producer, broker, processing engine, and sinks.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-taxonomy-of-delivery--processing-guarantees"&gt;1. Taxonomy of Delivery &amp;amp; Processing Guarantees&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;Term&lt;/th&gt;
 &lt;th&gt;Network Delivery&lt;/th&gt;
 &lt;th&gt;Processing Attempts&lt;/th&gt;
 &lt;th&gt;External Effect Guarantee&lt;/th&gt;
 &lt;th&gt;Typical Mechanisms&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;At most once&lt;/td&gt;
 &lt;td&gt;Messages may be lost&lt;/td&gt;
 &lt;td&gt;≤1&lt;/td&gt;
 &lt;td&gt;Missing effects possible&lt;/td&gt;
 &lt;td&gt;Fire-and-forget, no ack replay&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;At least once&lt;/td&gt;
 &lt;td&gt;Redelivery possible&lt;/td&gt;
 &lt;td&gt;≥1&lt;/td&gt;
 &lt;td&gt;Duplicated effects if not idempotent&lt;/td&gt;
 &lt;td&gt;Offsets/checkpoints + replay&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Exactly once (processing)&lt;/td&gt;
 &lt;td&gt;Redelivery possible&lt;/td&gt;
 &lt;td&gt;≥1&lt;/td&gt;
 &lt;td&gt;Each logical record causes effect once&lt;/td&gt;
 &lt;td&gt;Idempotence + dedupe or transactions + snapshots&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note the &lt;em&gt;processing&lt;/em&gt; dimension: we usually allow redelivery at the transport layer; the system masks duplicates before they become externally visible.&lt;/p&gt;</description></item><item><title>Latency-Aware Edge Inference Platforms: Engineering Consistent AI Experiences</title><link>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</guid><description>&lt;p&gt;Large language models, recommender systems, and computer vision pipelines increasingly run where users are: on kiosks, factory floors, AR headsets, connected vehicles, and retail shelves. But pushing models to the edge without a latency-aware strategy is a recipe for jittery UX, missed detections, and compliance headaches. This guide dissects what it takes to build an edge inference platform that meets strict latency budgets—even when networks wobble, models evolve weekly, and hardware varies wildly.&lt;/p&gt;</description></item><item><title>Designing CRDT-Powered Collaboration Platforms that Stay Consistent</title><link>https://lbenicio.dev/blog/designing-crdt-powered-collaboration-platforms-that-stay-consistent/</link><pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/designing-crdt-powered-collaboration-platforms-that-stay-consistent/</guid><description>&lt;p&gt;Real-time collaboration has shifted from a feature to a baseline expectation. Teams sketch ideas in shared whiteboards, edit code together, and annotate product specs while sitting on unreliable networks. Delivering that experience demands more than WebSockets and optimistic UI code. The hard part is consistency: making sure each participant converges on the same state despite offline edits, concurrent operations, and shaky connectivity. Conflict-free replicated data types (CRDTs) emerged as the workhorse that keeps these experiences coherent.&lt;/p&gt;</description></item></channel></rss>