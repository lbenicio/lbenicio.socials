<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Performance on Leonardo Benicio</title><link>https://lbenicio.dev/categories/performance/</link><description>Recent content in Performance on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 16 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/categories/performance/index.xml" rel="self" type="application/rss+xml"/><item><title>GPUDirect Storage in 2025: Optimizing the End-to-End Data Path</title><link>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</guid><description>&lt;p&gt;High-performance analytics and training pipelines increasingly hinge on how &lt;em&gt;fast&lt;/em&gt; and &lt;em&gt;efficiently&lt;/em&gt; data reaches GPU memory. Compute has outpaced I/O: a single multi-GPU node can sustain tens of TFLOPs while starved by a few misconfigured storage or copy stages. GPUDirect Storage (GDS) extends the GPUDirect family (peer-to-peer, RDMA) to allow DMA engines (NVMe, NIC) to move bytes directly between storage and GPU memory—bypassing redundant copies through host DRAM and reducing CPU intervention.&lt;/p&gt;</description></item><item><title>Lock-Free Data Structures: Concurrency Without the Wait</title><link>https://lbenicio.dev/blog/lock-free-data-structures-concurrency-without-the-wait/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/lock-free-data-structures-concurrency-without-the-wait/</guid><description>&lt;p&gt;Traditional locks have served concurrent programming for decades, but they come with costs: contention, priority inversion, and the ever-present risk of deadlock. Lock-free data structures offer an alternative—algorithms that guarantee system-wide progress even when individual threads stall. This post explores the theory, challenges, and practical implementations of lock-free programming.&lt;/p&gt;
&lt;h2 id="1-why-lock-free"&gt;1. Why Lock-Free?&lt;/h2&gt;
&lt;p&gt;Consider a simple counter shared among threads. With a mutex:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;pthread_mutex_lock&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;mutex);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;counter&lt;span style="color:#ff7b72;font-weight:bold"&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;pthread_mutex_unlock&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;mutex);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This works, but has problems:&lt;/p&gt;</description></item><item><title>Amdahl’s Law vs. Gustafson’s Law: What They Really Predict</title><link>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</guid><description>&lt;p&gt;Amdahl’s Law and Gustafson’s Law are often presented as opposites, but they model different scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amdahl assumes a fixed problem size and asks: how much faster can I make this workload with P processors when a fraction s is inherently serial? The upper bound is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_A(P) = \frac{1}{s + \frac{1-s}{P}}.$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gustafson assumes a fixed execution time and asks: given P processors, how much bigger can the problem become if parallel parts scale? The scaled speedup is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_G(P) = s + (1-s),P.$$&lt;/p&gt;</description></item><item><title>Memory Allocators: From malloc to Modern Arena Allocators</title><link>https://lbenicio.dev/blog/memory-allocators-from-malloc-to-modern-arena-allocators/</link><pubDate>Thu, 14 Sep 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/memory-allocators-from-malloc-to-modern-arena-allocators/</guid><description>&lt;p&gt;Memory allocation is one of those fundamental operations that most programmers take for granted. You call &lt;code&gt;malloc()&lt;/code&gt;, memory appears; you call &lt;code&gt;free()&lt;/code&gt;, it goes away. But beneath this simple interface lies a fascinating world of algorithms, trade-offs, and optimizations that can make or break your application&amp;rsquo;s performance. This post explores memory allocation from first principles to modern high-performance allocators.&lt;/p&gt;
&lt;h2 id="1-why-memory-allocation-matters"&gt;1. Why Memory Allocation Matters&lt;/h2&gt;
&lt;p&gt;Consider a web server handling thousands of requests per second. Each request might allocate dozens of objects: strings, buffers, data structures. If each allocation takes 1 microsecond, and a request needs 50 allocations, that&amp;rsquo;s 50 microseconds just in allocation overhead—potentially more than the actual request processing time.&lt;/p&gt;</description></item><item><title>Garbage Collection Algorithms: From Mark-and-Sweep to ZGC</title><link>https://lbenicio.dev/blog/garbage-collection-algorithms-from-mark-and-sweep-to-zgc/</link><pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/garbage-collection-algorithms-from-mark-and-sweep-to-zgc/</guid><description>&lt;p&gt;Manual memory management is powerful but error-prone. Forget to free memory and you leak; free too early and you corrupt. Garbage collection promises to solve this by automatically reclaiming unused memory. But this convenience comes with costs and trade-offs that every systems programmer should understand. This post explores garbage collection from first principles to cutting-edge concurrent collectors.&lt;/p&gt;
&lt;h2 id="1-why-garbage-collection"&gt;1. Why Garbage Collection?&lt;/h2&gt;
&lt;p&gt;Consider the challenges of manual memory management:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#ff7b72"&gt;void&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;process_request&lt;/span&gt;(Request&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; req) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;char&lt;/span&gt;&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; buffer &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;malloc&lt;/span&gt;(&lt;span style="color:#a5d6ff"&gt;1024&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Result&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; result &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;compute&lt;/span&gt;(req, buffer);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;if&lt;/span&gt; (result&lt;span style="color:#ff7b72;font-weight:bold"&gt;-&amp;gt;&lt;/span&gt;error) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#8b949e;font-style:italic"&gt;// Oops! Forgot to free buffer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;return&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;send_response&lt;/span&gt;(result);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;free&lt;/span&gt;(buffer);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;free&lt;/span&gt;(result); &lt;span style="color:#8b949e;font-style:italic"&gt;// Did compute() allocate this? Or is it static?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Common bugs include:&lt;/p&gt;</description></item><item><title>Branch Prediction and Speculative Execution: How Modern CPUs Gamble on the Future</title><link>https://lbenicio.dev/blog/branch-prediction-and-speculative-execution-how-modern-cpus-gamble-on-the-future/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/branch-prediction-and-speculative-execution-how-modern-cpus-gamble-on-the-future/</guid><description>&lt;p&gt;Modern CPUs are marvels of prediction. Every time your code branches—every if statement, every loop iteration, every function call—the processor makes a bet on what happens next. Get it right, and execution flows at full speed. Get it wrong, and the pipeline stalls while work is thrown away. Understanding branch prediction transforms how you think about code performance. This post explores the algorithms, trade-offs, and real-world implications of one of computing&amp;rsquo;s most important optimizations.&lt;/p&gt;</description></item></channel></rss>