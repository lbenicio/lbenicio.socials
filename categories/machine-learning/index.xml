<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on Leonardo Benicio</title><link>https://lbenicio.dev/categories/machine-learning/</link><description>Recent content in Machine Learning on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 26 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/categories/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Inside Vector Databases: Building Retrieval-Augmented Systems that Scale</title><link>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</link><pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</guid><description>&lt;p&gt;Vector search used to be a research curiosity. Today it sits in the critical path of customer support bots, developer copilots, fraud monitors, and every product marketing team experimenting with &amp;ldquo;retrieval-augmented&amp;rdquo; workflows. The excitement is deserved, but so is the sober engineering required to keep these systems accurate and available. Building a production vector database is more than storing tensors and calling cosine similarity. It demands a full stack of ingestion, indexing, storage management, failure handling, evaluation, and a constant feedback loop with the language models that consume those results.&lt;/p&gt;</description></item><item><title>Latency-Aware Edge Inference Platforms: Engineering Consistent AI Experiences</title><link>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</guid><description>&lt;p&gt;Large language models, recommender systems, and computer vision pipelines increasingly run where users are: on kiosks, factory floors, AR headsets, connected vehicles, and retail shelves. But pushing models to the edge without a latency-aware strategy is a recipe for jittery UX, missed detections, and compliance headaches. This guide dissects what it takes to build an edge inference platform that meets strict latency budgetsâ€”even when networks wobble, models evolve weekly, and hardware varies wildly.&lt;/p&gt;</description></item></channel></rss>