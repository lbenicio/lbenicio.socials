<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Systems on Leonardo Benicio</title><link>https://lbenicio.dev/categories/systems/</link><description>Recent content in Systems on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 21 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/categories/systems/index.xml" rel="self" type="application/rss+xml"/><item><title>Database Internals: Storage Engines, Transactions, and Recovery</title><link>https://lbenicio.dev/blog/database-internals-storage-engines-transactions-and-recovery/</link><pubDate>Sun, 21 Dec 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/database-internals-storage-engines-transactions-and-recovery/</guid><description>&lt;p&gt;Databases are much more than SQL parsers and client libraries — their core is a storage engine that durably stores and efficiently retrieves data while preserving the guarantees applications depend upon. This article unpacks how modern databases manage on-disk data structures, coordinate concurrent access, provide transactional semantics, and recover from crashes. We&amp;rsquo;ll look at B-trees and LSM-trees, the write-ahead log, MVCC and isolation levels, recovery algorithms, and practical tuning advice for real-world systems.&lt;/p&gt;</description></item><item><title>CPU Microarchitecture: Pipelines, Out-of-Order Execution, and Modern Performance</title><link>https://lbenicio.dev/blog/cpu-microarchitecture-pipelines-out-of-order-execution-and-modern-performance/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-microarchitecture-pipelines-out-of-order-execution-and-modern-performance/</guid><description>&lt;p&gt;Modern CPUs are marvels of engineering designed to extract instruction-level parallelism (ILP) from sequential programs while hiding long latencies — memory, multiplies, or long dependency chains. To understand why some code runs orders of magnitude faster than other code that &amp;ldquo;does the same work,&amp;rdquo; you need to understand microarchitectural components such as pipelines, superscalar issue, branch prediction, out-of-order (OoO) execution, register renaming, reorder buffers, and vector (SIMD) units. This article takes a practical tour of these features, how they affect instruction throughput and latency, and pragmatic tips for writing high-performance code.&lt;/p&gt;</description></item><item><title>Distributed Systems: Consensus, Consistency, and Fault Tolerance</title><link>https://lbenicio.dev/blog/distributed-systems-consensus-consistency-and-fault-tolerance/</link><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/distributed-systems-consensus-consistency-and-fault-tolerance/</guid><description>&lt;p&gt;Distributed systems are deceptively simple to describe and maddeningly difficult to build. When a single process is replaced by a collection of cooperating processes that communicate over unreliable networks, familiar assumptions break: partial failure becomes the norm, time is not globally synchronized, and correctness requires making explicit trade-offs. This post covers the foundations you need to reason about building correct, robust, and performant distributed systems: failure models, consensus and leader election, replication and consistency models, gossip and membership, CRDTs for conflict-free replication, testing strategies (including Jepsen-style fault injection), and operational best practices.&lt;/p&gt;</description></item><item><title>The Quiet Calculus of Probabilistic Commutativity</title><link>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</link><pubDate>Sat, 27 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Eventual consistency dominates many internet-scale systems, but reasoning about concurrency under minimal coordination remains ad hoc. This post introduces &amp;ldquo;probabilistic commutativity&amp;rdquo; — a lightweight calculus for reasoning about whether concurrent operations, under reasonable stochastic assumptions about ordering and visibility delays, are likely to commute in practice. Probabilistic commutativity offers an intermediate lens between strict algebraic commutativity and empirical test-driven guarantees, enabling low-overhead coordination strategies and probabilistic correctness arguments for producing practically consistent distributed services.&lt;/p&gt;</description></item><item><title>Memory Allocation and Garbage Collection: How Programs Manage Memory</title><link>https://lbenicio.dev/blog/memory-allocation-and-garbage-collection-how-programs-manage-memory/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/memory-allocation-and-garbage-collection-how-programs-manage-memory/</guid><description>&lt;p&gt;Every program needs memory, and every byte eventually becomes garbage. Between allocation and collection lies a fascinating world of algorithms, data structures, and engineering trade-offs that profoundly affect application performance. Whether you&amp;rsquo;re debugging memory leaks, optimizing allocation patterns, or choosing between languages, understanding memory management internals gives you the mental models to make better decisions.&lt;/p&gt;
&lt;h2 id="1-the-memory-landscape"&gt;1. The Memory Landscape&lt;/h2&gt;
&lt;p&gt;Before diving into allocation strategies, let&amp;rsquo;s understand the terrain.&lt;/p&gt;
&lt;h3 id="11-virtual-address-space-layout"&gt;1.1 Virtual Address Space Layout&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Typical Process Memory Layout (Linux x86-64):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────────────────────┐ 0x7FFFFFFFFFFF (high)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Stack │ ← Grows downward
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ↓ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Unmapped Region │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ↑ │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Heap │ ← Grows upward
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ BSS Segment │ ← Uninitialized globals
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Data Segment │ ← Initialized globals
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─────────────────────────────────┤
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Text Segment │ ← Program code (read-only)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─────────────────────────────────┘ 0x400000 (low)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The stack and heap grow toward each other from opposite ends of the address space. The stack handles function call frames automatically, while the heap requires explicit management—either by the programmer or by a garbage collector.&lt;/p&gt;</description></item><item><title>Write-Ahead Logging: The Unsung Hero of Database Durability</title><link>https://lbenicio.dev/blog/write-ahead-logging-the-unsung-hero-of-database-durability/</link><pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/write-ahead-logging-the-unsung-hero-of-database-durability/</guid><description>&lt;p&gt;Every database makes a promise: your committed data will survive crashes, power failures, and hardware hiccups. This promise—durability—seems simple until you consider the physics. Disks are slow. Memory is volatile. Writes can be reordered. Yet somehow, databases deliver both durability and performance. The secret weapon? Write-ahead logging.&lt;/p&gt;
&lt;h2 id="1-the-durability-problem"&gt;1. The Durability Problem&lt;/h2&gt;
&lt;p&gt;Consider what happens when you execute &lt;code&gt;UPDATE accounts SET balance = balance - 100 WHERE id = 42&lt;/code&gt;. The database must:&lt;/p&gt;</description></item><item><title>Bloom Filters and Probabilistic Data Structures: Trading Certainty for Speed</title><link>https://lbenicio.dev/blog/bloom-filters-and-probabilistic-data-structures-trading-certainty-for-speed/</link><pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/bloom-filters-and-probabilistic-data-structures-trading-certainty-for-speed/</guid><description>&lt;p&gt;Sometimes the right answer is &amp;ldquo;probably yes.&amp;rdquo; In a world obsessed with correctness, probabilistic data structures offer a heretical bargain: give up certainty, and in return receive orders-of-magnitude improvements in memory, speed, or both. This post explores the theory and practice behind Bloom filters, Count-Min sketches, HyperLogLog, and their variants—structures that power everything from database query optimization to network traffic analysis at scale.&lt;/p&gt;
&lt;h2 id="1-the-case-for-uncertainty"&gt;1. The Case for Uncertainty&lt;/h2&gt;
&lt;p&gt;Traditional data structures promise exact answers. A hash set tells you definitively whether an element exists. A counter tells you precisely how many times something occurred. But exactness has a cost: memory consumption grows with the number of distinct elements, and lookups may require following chains or probing sequences.&lt;/p&gt;</description></item><item><title>Lock-Free Data Structures: Concurrency Without the Wait</title><link>https://lbenicio.dev/blog/lock-free-data-structures-concurrency-without-the-wait/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/lock-free-data-structures-concurrency-without-the-wait/</guid><description>&lt;p&gt;Traditional locks have served concurrent programming for decades, but they come with costs: contention, priority inversion, and the ever-present risk of deadlock. Lock-free data structures offer an alternative—algorithms that guarantee system-wide progress even when individual threads stall. This post explores the theory, challenges, and practical implementations of lock-free programming.&lt;/p&gt;
&lt;h2 id="1-why-lock-free"&gt;1. Why Lock-Free?&lt;/h2&gt;
&lt;p&gt;Consider a simple counter shared among threads. With a mutex:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;pthread_mutex_lock&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;mutex);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;counter&lt;span style="color:#ff7b72;font-weight:bold"&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;pthread_mutex_unlock&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;mutex);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This works, but has problems:&lt;/p&gt;</description></item><item><title>Concurrency Primitives and Synchronization: From Spinlocks to Lock-Free Data Structures</title><link>https://lbenicio.dev/blog/concurrency-primitives-and-synchronization-from-spinlocks-to-lock-free-data-structures/</link><pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/concurrency-primitives-and-synchronization-from-spinlocks-to-lock-free-data-structures/</guid><description>&lt;p&gt;Concurrent programming transforms sequential code into parallel execution, unlocking the power of modern multicore processors. But with parallelism comes the challenge of coordinating access to shared resources. Race conditions lurk in code that appears correct, and subtle bugs emerge only under specific timing conditions. Understanding synchronization primitives from the hardware level up through high-level abstractions provides the foundation for writing correct, efficient concurrent programs.&lt;/p&gt;
&lt;h2 id="1-the-concurrency-challenge"&gt;1. The Concurrency Challenge&lt;/h2&gt;
&lt;p&gt;Why synchronization matters and what problems it solves.&lt;/p&gt;</description></item><item><title>Unicode and Character Encoding: From ASCII to UTF-8 and Beyond</title><link>https://lbenicio.dev/blog/unicode-and-character-encoding-from-ascii-to-utf-8-and-beyond/</link><pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/unicode-and-character-encoding-from-ascii-to-utf-8-and-beyond/</guid><description>&lt;p&gt;Text seems simple until you try to handle it correctly. A single question—&amp;ldquo;how many characters are in this string?&amp;quot;—can have multiple valid answers depending on what you mean by &amp;ldquo;character.&amp;rdquo; Understanding Unicode and character encoding is essential for any programmer working with internationalized text, file formats, network protocols, or databases.&lt;/p&gt;
&lt;h2 id="1-the-history-of-character-encoding"&gt;1. The History of Character Encoding&lt;/h2&gt;
&lt;p&gt;Before we can understand where we are, we need to know how we got here.&lt;/p&gt;</description></item><item><title>File Systems and Storage Internals: How Data Persists on Disk</title><link>https://lbenicio.dev/blog/file-systems-and-storage-internals-how-data-persists-on-disk/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/file-systems-and-storage-internals-how-data-persists-on-disk/</guid><description>&lt;p&gt;Every file you save, every application you install, every database record—all must survive power failures and system crashes. File systems provide this durability guarantee while making storage appear as a simple hierarchy of named files and directories. Behind this abstraction lies sophisticated machinery for organizing billions of bytes, recovering from failures, and optimizing access patterns. Understanding file system internals illuminates why some operations are fast and others slow, why disks fill up unexpectedly, and how your data survives the unexpected.&lt;/p&gt;</description></item><item><title>Memory Allocators: From malloc to Modern Arena Allocators</title><link>https://lbenicio.dev/blog/memory-allocators-from-malloc-to-modern-arena-allocators/</link><pubDate>Thu, 14 Sep 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/memory-allocators-from-malloc-to-modern-arena-allocators/</guid><description>&lt;p&gt;Memory allocation is one of those fundamental operations that most programmers take for granted. You call &lt;code&gt;malloc()&lt;/code&gt;, memory appears; you call &lt;code&gt;free()&lt;/code&gt;, it goes away. But beneath this simple interface lies a fascinating world of algorithms, trade-offs, and optimizations that can make or break your application&amp;rsquo;s performance. This post explores memory allocation from first principles to modern high-performance allocators.&lt;/p&gt;
&lt;h2 id="1-why-memory-allocation-matters"&gt;1. Why Memory Allocation Matters&lt;/h2&gt;
&lt;p&gt;Consider a web server handling thousands of requests per second. Each request might allocate dozens of objects: strings, buffers, data structures. If each allocation takes 1 microsecond, and a request needs 50 allocations, that&amp;rsquo;s 50 microseconds just in allocation overhead—potentially more than the actual request processing time.&lt;/p&gt;</description></item><item><title>TCP Congestion Control: From Slow Start to BBR</title><link>https://lbenicio.dev/blog/tcp-congestion-control-from-slow-start-to-bbr/</link><pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tcp-congestion-control-from-slow-start-to-bbr/</guid><description>&lt;p&gt;Every time you load a webpage, stream a video, or download a file, TCP congestion control algorithms work invisibly to maximize throughput while preventing network collapse. These algorithms represent decades of research into one of networking&amp;rsquo;s hardest problems: how do independent senders share a network fairly without overwhelming it? Let&amp;rsquo;s explore how TCP congestion control evolved from simple beginnings to sophisticated model-based approaches.&lt;/p&gt;
&lt;h2 id="1-the-congestion-problem"&gt;1. The Congestion Problem&lt;/h2&gt;
&lt;p&gt;Without congestion control, networks collapse under load.&lt;/p&gt;</description></item><item><title>Floating Point: How Computers Represent Real Numbers</title><link>https://lbenicio.dev/blog/floating-point-how-computers-represent-real-numbers/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/floating-point-how-computers-represent-real-numbers/</guid><description>&lt;p&gt;Every programmer eventually encounters the classic puzzle: why does &lt;code&gt;0.1 + 0.2&lt;/code&gt; not equal &lt;code&gt;0.3&lt;/code&gt;? The answer lies in how computers represent real numbers using floating point arithmetic. Understanding IEEE 754 floating point is essential for anyone writing numerical code, financial software, scientific simulations, or graphics applications.&lt;/p&gt;
&lt;h2 id="1-the-challenge-of-representing-real-numbers"&gt;1. The Challenge of Representing Real Numbers&lt;/h2&gt;
&lt;p&gt;Computers work with finite binary representations, but real numbers are infinite.&lt;/p&gt;
&lt;h3 id="11-the-fundamental-problem"&gt;1.1 The Fundamental Problem&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Integers are straightforward:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;42 in binary = 101010
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;-7 in binary (two&amp;#39;s complement, 8-bit) = 11111001
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;But real numbers have infinite precision:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;π = 3.14159265358979323846...
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;1/3 = 0.33333333... (repeating)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Even simple decimals can be infinite in binary:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;0.1 (decimal) = 0.0001100110011... (repeating in binary)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-why-not-fixed-point"&gt;1.2 Why Not Fixed Point?&lt;/h3&gt;
&lt;p&gt;Fixed point representations dedicate a fixed number of bits to the integer and fractional parts:&lt;/p&gt;</description></item><item><title>Garbage Collection Algorithms: From Mark-and-Sweep to ZGC</title><link>https://lbenicio.dev/blog/garbage-collection-algorithms-from-mark-and-sweep-to-zgc/</link><pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/garbage-collection-algorithms-from-mark-and-sweep-to-zgc/</guid><description>&lt;p&gt;Manual memory management is powerful but error-prone. Forget to free memory and you leak; free too early and you corrupt. Garbage collection promises to solve this by automatically reclaiming unused memory. But this convenience comes with costs and trade-offs that every systems programmer should understand. This post explores garbage collection from first principles to cutting-edge concurrent collectors.&lt;/p&gt;
&lt;h2 id="1-why-garbage-collection"&gt;1. Why Garbage Collection?&lt;/h2&gt;
&lt;p&gt;Consider the challenges of manual memory management:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#ff7b72"&gt;void&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;process_request&lt;/span&gt;(Request&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; req) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;char&lt;/span&gt;&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; buffer &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;malloc&lt;/span&gt;(&lt;span style="color:#a5d6ff"&gt;1024&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Result&lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; result &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;compute&lt;/span&gt;(req, buffer);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;if&lt;/span&gt; (result&lt;span style="color:#ff7b72;font-weight:bold"&gt;-&amp;gt;&lt;/span&gt;error) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#8b949e;font-style:italic"&gt;// Oops! Forgot to free buffer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#ff7b72"&gt;return&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;send_response&lt;/span&gt;(result);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;free&lt;/span&gt;(buffer);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#d2a8ff;font-weight:bold"&gt;free&lt;/span&gt;(result); &lt;span style="color:#8b949e;font-style:italic"&gt;// Did compute() allocate this? Or is it static?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Common bugs include:&lt;/p&gt;</description></item><item><title>CPU Caches and Cache Coherence: The Memory Hierarchy That Makes Modern Computing Fast</title><link>https://lbenicio.dev/blog/cpu-caches-and-cache-coherence-the-memory-hierarchy-that-makes-modern-computing-fast/</link><pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-caches-and-cache-coherence-the-memory-hierarchy-that-makes-modern-computing-fast/</guid><description>&lt;p&gt;Modern CPUs can execute billions of instructions per second, but main memory takes hundreds of cycles to respond. Without caches, processors would spend most of their time waiting for data. The cache hierarchy is one of the most important innovations in computer architecture, and understanding it is essential for writing high-performance software.&lt;/p&gt;
&lt;h2 id="1-the-memory-wall-problem"&gt;1. The Memory Wall Problem&lt;/h2&gt;
&lt;p&gt;The fundamental challenge that caches solve.&lt;/p&gt;
&lt;h3 id="11-the-speed-gap"&gt;1.1 The Speed Gap&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Component Access Time Relative Speed
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;─────────────────────────────────────────────────
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;CPU Register ~0.3 ns 1x (baseline)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L1 Cache ~1 ns ~3x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L2 Cache ~3-4 ns ~10x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;L3 Cache ~10-20 ns ~30-60x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Main Memory ~50-100 ns ~150-300x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;NVMe SSD ~20,000 ns ~60,000x slower
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;HDD ~10,000,000 ns ~30,000,000x slower
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-why-the-gap-exists"&gt;1.2 Why the Gap Exists&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Memory technology tradeoffs:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;SRAM (caches):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Fast: 6 transistors per bit
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Expensive: ~100x cost per bit vs DRAM
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Power hungry
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─ Low density
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;DRAM (main memory):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Slow: 1 transistor + 1 capacitor per bit
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Cheap: high density
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├─ Needs refresh (capacitors leak)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└─ Better power per bit
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="13-the-solution-caching"&gt;1.3 The Solution: Caching&lt;/h3&gt;
&lt;p&gt;Caches exploit two key principles:&lt;/p&gt;</description></item><item><title>Virtual Memory and Page Tables: How Modern Systems Manage Memory</title><link>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-modern-systems-manage-memory/</link><pubDate>Thu, 19 May 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-modern-systems-manage-memory/</guid><description>&lt;p&gt;Every process believes it has exclusive access to a vast, contiguous memory space. This elegant illusion—virtual memory—is one of the most important abstractions in computing. Behind it lies a sophisticated system of page tables, TLBs, and hardware-software cooperation that enables memory isolation, efficient sharing, and seemingly infinite memory. Let&amp;rsquo;s explore how it all works.&lt;/p&gt;
&lt;h2 id="1-the-problem-why-virtual-memory"&gt;1. The Problem: Why Virtual Memory?&lt;/h2&gt;
&lt;p&gt;Before virtual memory, programs used physical addresses directly. This created serious problems.&lt;/p&gt;</description></item><item><title>Process Scheduling and Context Switching: How Operating Systems Share the CPU</title><link>https://lbenicio.dev/blog/process-scheduling-and-context-switching-how-operating-systems-share-the-cpu/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/process-scheduling-and-context-switching-how-operating-systems-share-the-cpu/</guid><description>&lt;p&gt;A single CPU core can only execute one instruction stream at a time, yet modern systems run hundreds of processes simultaneously. This illusion of parallelism requires the operating system to rapidly switch between processes, giving each a slice of CPU time. The scheduler—the component that decides who runs next—profoundly affects system responsiveness, throughput, and fairness. Understanding scheduling reveals why your interactive applications feel smooth or sluggish and why some workloads perform better than others.&lt;/p&gt;</description></item><item><title>Branch Prediction and Speculative Execution: How Modern CPUs Gamble on the Future</title><link>https://lbenicio.dev/blog/branch-prediction-and-speculative-execution-how-modern-cpus-gamble-on-the-future/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/branch-prediction-and-speculative-execution-how-modern-cpus-gamble-on-the-future/</guid><description>&lt;p&gt;Modern CPUs are marvels of prediction. Every time your code branches—every if statement, every loop iteration, every function call—the processor makes a bet on what happens next. Get it right, and execution flows at full speed. Get it wrong, and the pipeline stalls while work is thrown away. Understanding branch prediction transforms how you think about code performance. This post explores the algorithms, trade-offs, and real-world implications of one of computing&amp;rsquo;s most important optimizations.&lt;/p&gt;</description></item><item><title>Virtual Memory and Page Tables: How Operating Systems Manage Memory</title><link>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-operating-systems-manage-memory/</link><pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/virtual-memory-and-page-tables-how-operating-systems-manage-memory/</guid><description>&lt;p&gt;Every process believes it has the entire machine to itself. It sees a vast, contiguous address space starting from zero, completely isolated from other processes. This illusion is virtual memory—one of the most important abstractions in computing. Understanding how operating systems and hardware collaborate to maintain this illusion reveals fundamental insights about performance, security, and system design.&lt;/p&gt;
&lt;h2 id="1-the-need-for-virtual-memory"&gt;1. The Need for Virtual Memory&lt;/h2&gt;
&lt;p&gt;Before virtual memory, programming was a constant juggling act.&lt;/p&gt;</description></item><item><title>B-Trees and LSM-Trees: The Foundations of Modern Storage Engines</title><link>https://lbenicio.dev/blog/b-trees-and-lsm-trees-the-foundations-of-modern-storage-engines/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/b-trees-and-lsm-trees-the-foundations-of-modern-storage-engines/</guid><description>&lt;p&gt;Every database faces the same fundamental challenge: how do you organize data on disk so that both reads and writes are fast? Two data structures have emerged as the dominant answers—B-Trees and LSM-Trees. Understanding their trade-offs is essential for anyone building or operating data-intensive systems. This post explores both in depth, from their internal mechanics to their real-world implementations.&lt;/p&gt;
&lt;h2 id="1-the-storage-engine-problem"&gt;1. The Storage Engine Problem&lt;/h2&gt;
&lt;p&gt;Before diving into specific data structures, let&amp;rsquo;s understand what we&amp;rsquo;re optimizing for.&lt;/p&gt;</description></item><item><title>CPU Caches and Memory Hierarchy: The Hidden Architecture Behind Performance</title><link>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cpu-caches-and-memory-hierarchy-the-hidden-architecture-behind-performance/</guid><description>&lt;p&gt;The gap between processor speed and memory speed is one of the defining challenges of modern computing. While CPUs can execute billions of operations per second, main memory takes hundreds of cycles to respond to a single request. CPU caches bridge this gap through a hierarchy of progressively larger and slower memories that exploit the patterns in how programs access data. Understanding cache behavior transforms how you think about algorithm design, data structure layout, and system performance.&lt;/p&gt;</description></item><item><title>System Calls: The Gateway Between User Space and Kernel</title><link>https://lbenicio.dev/blog/system-calls-the-gateway-between-user-space-and-kernel/</link><pubDate>Sun, 18 Apr 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/system-calls-the-gateway-between-user-space-and-kernel/</guid><description>&lt;p&gt;Every time your program opens a file, allocates memory, or sends a network packet, it crosses an invisible boundary. User programs cannot directly access hardware or kernel data structures—they must ask the operating system to do it for them through system calls. Understanding this interface is fundamental to systems programming and helps explain performance characteristics, security boundaries, and the design of operating systems themselves.&lt;/p&gt;
&lt;h2 id="1-the-user-kernel-boundary"&gt;1. The User-Kernel Boundary&lt;/h2&gt;
&lt;p&gt;Modern operating systems divide the world into two privilege levels.&lt;/p&gt;</description></item><item><title>Network Sockets and the TCP/IP Stack: How Data Travels Across Networks</title><link>https://lbenicio.dev/blog/network-sockets-and-the-tcp/ip-stack-how-data-travels-across-networks/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/network-sockets-and-the-tcp/ip-stack-how-data-travels-across-networks/</guid><description>&lt;p&gt;Every web request, database query, and API call travels through the network stack. The simple act of opening a connection hides layers of protocol machinery handling packet routing, reliable delivery, congestion control, and flow management. Understanding how sockets work and how data traverses the TCP/IP stack illuminates why networks behave as they do and how to build efficient networked applications.&lt;/p&gt;
&lt;h2 id="1-the-network-stack-overview"&gt;1. The Network Stack Overview&lt;/h2&gt;
&lt;p&gt;Before diving into details, let&amp;rsquo;s see the complete picture.&lt;/p&gt;</description></item><item><title>Compiler Optimizations: From Source Code to Fast Machine Code</title><link>https://lbenicio.dev/blog/compiler-optimizations-from-source-code-to-fast-machine-code/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/compiler-optimizations-from-source-code-to-fast-machine-code/</guid><description>&lt;p&gt;When you compile your code with &lt;code&gt;-O2&lt;/code&gt; or &lt;code&gt;-O3&lt;/code&gt;, something magical happens. The compiler applies dozens of optimization passes that can make your program run 10x faster—or more. Understanding these optimizations helps you write faster code and debug mysterious performance issues. Let&amp;rsquo;s explore how modern compilers transform source code into efficient machine code.&lt;/p&gt;
&lt;h2 id="1-the-compilation-pipeline"&gt;1. The Compilation Pipeline&lt;/h2&gt;
&lt;p&gt;Before diving into optimizations, let&amp;rsquo;s understand where they happen.&lt;/p&gt;
&lt;h3 id="11-compiler-phases"&gt;1.1 Compiler Phases&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Source Code
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Front-end │ Parsing, type checking, AST generation
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ (Language- │ C, C++, Rust, Swift → IR
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ specific) │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└────────┬────────┘
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Middle-end │ ★ MOST OPTIMIZATIONS HAPPEN HERE ★
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ (IR-based │ Constant folding, inlining, loop opts
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ optimization) │ Dead code elimination, vectorization
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└────────┬────────┘
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;┌─────────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ Back-end │ Instruction selection, register allocation
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ (Target- │ Instruction scheduling, peephole opts
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ specific) │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└────────┬────────┘
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; │
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ▼
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Machine Code
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-intermediate-representation-ir"&gt;1.2 Intermediate Representation (IR)&lt;/h3&gt;
&lt;p&gt;LLVM IR is a popular intermediate representation:&lt;/p&gt;</description></item><item><title>Consistent Hashing: Distributing Data Across Dynamic Clusters</title><link>https://lbenicio.dev/blog/consistent-hashing-distributing-data-across-dynamic-clusters/</link><pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/consistent-hashing-distributing-data-across-dynamic-clusters/</guid><description>&lt;p&gt;When you add a server to your distributed cache, what happens to all the cached data? With naive hashing, almost everything moves—a catastrophic reshuffling that defeats the purpose of caching. Consistent hashing solves this elegantly: only K/N keys need to move, where K is total keys and N is the number of servers. This simple idea underpins some of the most scalable systems ever built. Let&amp;rsquo;s explore how it works and why it matters.&lt;/p&gt;</description></item></channel></rss>